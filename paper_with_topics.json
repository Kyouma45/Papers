[
  {
    "Title": "$XX^{t}$ Can Be Faster",
    "Reading Status": "Want to Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2505.09814v1",
    "Topics": [],
    "Description": "We present RXTX, a new algorithm for computing the product of matrix by its transpose $XX^{t}$ for $X\\in \\mathbb{R}^{n\\times m}$. RXTX uses $5\\%$ fewer multiplications and $5\\%$ fewer operations (additions and multiplications) than State-of-the-Art algorithms. Note that the accelerations not only holds asymptotically for large matrices with $n \\rightarrow \\infty$, but also for small matrices including $n = 4$. The algorithm was discovered by combining Machine Learning-based search methods with Combinatorial Optimization.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Dmitry Rybin",
      "Yushun Zhang",
      "Zhi-Quan Luo"
    ],
    "Date Updated": "2025-05-16",
    "Comment": "improved presentation",
    "PDF Link": "http://arxiv.org/pdf/2505.09814v2",
    "Primary Category": "cs.DS",
    "All Categories": [
      "cs.DS",
      "cs.AI",
      "cs.LG",
      "cs.SC",
      "68Q25, 68T20",
      "F.2.1; I.1.2"
    ],
    "Version": "2"
  },
  {
    "Title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-09",
    "Link": "https://arxiv.org/abs/2501.06252",
    "Topics": [],
    "Description": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce $\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, $\\text{Transformer}^2$ employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. $\\text{Transformer}^2$ represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Qi Sun",
      "Edoardo Cetin",
      "Yujin Tang"
    ],
    "Date Updated": "2025-01-24",
    "Comment": "To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025)",
    "PDF Link": "http://arxiv.org/pdf/2501.06252v3",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "Version": "3"
  },
  {
    "Title": "52B to 1T: Lessons Learned via Tele-FLM Series",
    "Reading Status": "Read",
    "Date Published": "2024-07-03",
    "Link": "https://arxiv.org/abs/2407.02783",
    "Topics": [],
    "Description": "Large Language Models (LLMs) represent a significant stride toward Artificial General Intelligence. As scaling laws underscore the potential of increasing model sizes, the academic community has intensified its investigations into LLMs with capacities exceeding 50 billion parameters. This technical report builds on our prior work with Tele-FLM (also known as FLM-2), a publicly available 52-billion-parameter model. We delve into two primary areas: we first discuss our observation of Supervised Fine-tuning (SFT) on Tele-FLM-52B, which supports the \"less is more\" approach for SFT data construction; second, we demonstrate our experiments and analyses on the best practices for progressively growing a model from 52 billion to 102 billion, and subsequently to 1 trillion parameters. We will open-source a 1T model checkpoint, namely Tele-FLM-1T, to advance further training and research. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Xiang Li",
      "Yiqun Yao",
      "Xin Jiang",
      "Xuezhi Fang",
      "Chao Wang",
      "Xinzhang Liu",
      "Zihan Wang",
      "Yu Zhao",
      "Xin Wang",
      "Yuyao Huang",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zheng Zhang",
      "Bo Zhao",
      "Aixin Sun",
      "Yequan Wang",
      "Zhongjiang He",
      "Zhongyuan Wang",
      "Xuelong Li",
      "Tiejun Huang"
    ],
    "Date Updated": "2024-07-03",
    "Comment": "For the Tele-FLM-52B tech report, see also 2404.16645",
    "PDF Link": "http://arxiv.org/pdf/2407.02783v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/",
    "Topics": [
      "AlphaEvolve"
    ],
    "Description": "Large language models (LLMs) are remarkably versatile. They can summarize documents, generate code or even brainstorm new ideas. And now we’ve expanded these capabilities to target fundamental and highly complex problems in mathematics and modern computing.\n\nToday, we’re announcing AlphaEvolve, an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our Gemini models with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas.\n\nAlphaEvolve enhanced the efficiency of Google's data centers, chip design and AI training processes — including training the large language models underlying AlphaEvolve itself. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas.",
    "Month": "2025-08",
    "Date Read": "2025-08-18",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [],
    "Date Updated": null,
    "Comment": NaN,
    "PDF Link": NaN,
    "Primary Category": NaN,
    "All Categories": [],
    "Version": NaN
  },
  {
    "Title": "Aya 23: Open Weight Release to Further Multilingual Progress",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2405.15032",
    "Topics": [
      "Multilingual",
      "Technical Report"
    ],
    "Description": "This technical report introduces Aya 23, a family of multilingual language models. Aya 23 builds on the recent release of the Aya model (Üstün et al., 2024), focusing on pairing a highly performant pre-trained model with the recently released Aya collection (Singh et al., 2024). The result is a powerful multilingual large language model serving 23 languages, expanding state-of-art language modeling capabilities to approximately half of the world's population. The Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs breadth, exploring the impact of allocating more capacity to fewer languages that are included during pre-training. Aya 23 outperforms both previous massively multilingual models like Aya 101 for the languages it covers, as well as widely used models like Gemma, Mistral and Mixtral on an extensive range of discriminative and generative tasks. We release the open weights for both the 8B and 35B models as part of our continued commitment for expanding access to multilingual progress. ",
    "Month": "2025-08",
    "Date Read": "2025-08-18",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Viraat Aryabumi",
      "John Dang",
      "Dwarak Talupuru",
      "Saurabh Dash",
      "David Cairuz",
      "Hangyu Lin",
      "Bharat Venkitesh",
      "Madeline Smith",
      "Jon Ander Campos",
      "Yi Chern Tan",
      "Kelly Marchisio",
      "Max Bartolo",
      "Sebastian Ruder",
      "Acyr Locatelli",
      "Julia Kreutzer",
      "Nick Frosst",
      "Aidan Gomez",
      "Phil Blunsom",
      "Marzieh Fadaee",
      "Ahmet Üstün",
      "Sara Hooker"
    ],
    "Date Updated": "2024-05-31",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2405.15032v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "Reading Status": "Read",
    "Date Published": "2018-10-11",
    "Link": "https://arxiv.org/abs/1810.04805",
    "Topics": [
      "Technical Report",
      "BERT",
      "Encoders"
    ],
    "Description": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jacob Devlin",
      "Ming-Wei Chang",
      "Kenton Lee",
      "Kristina Toutanova"
    ],
    "Date Updated": "2019-05-24",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/1810.04805v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning\n  for Automatic Speech Recognition",
    "Reading Status": "Read",
    "Date Published": "2021-09-27",
    "Link": "https://arxiv.org/abs/2109.13226",
    "Topics": [
      "Audio"
    ],
    "Description": "We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks.",
    "Month": "2025-08",
    "Date Read": "2025-09-01",
    "Date Added": "2025-08-30",
    "WeekDay": "Saturday",
    "Week": 35,
    "Authors": [
      "Yu Zhang",
      "Daniel S. Park",
      "Wei Han",
      "James Qin",
      "Anmol Gulati",
      "Joel Shor",
      "Aren Jansen",
      "Yuanzhong Xu",
      "Yanping Huang",
      "Shibo Wang",
      "Zongwei Zhou",
      "Bo Li",
      "Min Ma",
      "William Chan",
      "Jiahui Yu",
      "Yongqiang Wang",
      "Liangliang Cao",
      "Khe Chai Sim",
      "Bhuvana Ramabhadran",
      "Tara N. Sainath",
      "Françoise Beaufays",
      "Zhifeng Chen",
      "Quoc V. Le",
      "Chung-Cheng Chiu",
      "Ruoming Pang",
      "Yonghui Wu"
    ],
    "Date Updated": "2022-07-21",
    "Comment": "14 pages, 7 figures, 13 tables; v2: minor corrections, reference\n  baselines and bibliography updated; v3: corrections based on reviewer\n  feedback, bibliography updated",
    "PDF Link": "http://arxiv.org/pdf/2109.13226v3",
    "Primary Category": "eess.AS",
    "All Categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "Version": "3"
  },
  {
    "Title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-11",
    "Link": "https://arxiv.org/abs/2501.06590",
    "Topics": [],
    "Description": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Xiangru Tang",
      "Tianyu Hu",
      "Muyang Ye",
      "Yanjun Shao",
      "Xunjian Yin",
      "Siru Ouyang",
      "Wangchunshu Zhou",
      "Pan Lu",
      "Zhuosheng Zhang",
      "Yilun Zhao",
      "Arman Cohan",
      "Mark Gerstein"
    ],
    "Date Updated": "2025-01-11",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.06590v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity",
    "Reading Status": "Read",
    "Date Published": "2025-06-10",
    "Link": "https://arxiv.org/abs/2506.09250",
    "Topics": [
      "COT"
    ],
    "Description": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit \"accuracy collapse\" on planning puzzles beyond certain complexity thresholds. We demonstrate that their findings primarily reflect experimental design limitations rather than fundamental reasoning failures. Our analysis reveals three critical issues: (1) Tower of Hanoi experiments risk exceeding model output token limits, with models explicitly acknowledging these constraints in their outputs; (2) The authors' automated evaluation framework fails to distinguish between reasoning failures and practical constraints, leading to misclassification of model capabilities; (3) Most concerningly, their River Crossing benchmarks include mathematically impossible instances for N > 5 due to insufficient boat capacity, yet models are scored as failures for not solving these unsolvable problems. When we control for these experimental artifacts, by requesting generating functions instead of exhaustive move lists, preliminary experiments across multiple models indicate high accuracy on Tower of Hanoi instances previously reported as complete failures. These findings highlight the importance of careful experimental design when evaluating AI reasoning capabilities.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "A. Lawsen"
    ],
    "Date Updated": "2025-06-16",
    "Comment": "Comment on: arXiv:2506.06941 Latest version removes Claude as a\n  co-author, in line with arXiv policies, it also corrects mistakes in sections\n  4 and 6 of the original submission, as well as several typographical errors",
    "PDF Link": "http://arxiv.org/pdf/2506.09250v2",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "Decoding Speculative Decoding",
    "Reading Status": "Read",
    "Date Published": "2024-02-02",
    "Link": "https://arxiv.org/abs/2402.01528",
    "Topics": [
      "Speculative Decoding"
    ],
    "Description": "Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models and our approach generalizes further to all LLaMA models (1/2/3.1) and supervised fine-tuned models. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Minghao Yan",
      "Saurabh Agarwal",
      "Shivaram Venkataraman"
    ],
    "Date Updated": "2025-02-05",
    "Comment": "Proceedings of the 2025 Conference of the North American Chapter of\n  the Association for Computational Linguistics: Human Language Technologies\n  (NAACL 2025)",
    "PDF Link": "http://arxiv.org/pdf/2402.01528v4",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.CL"
    ],
    "Version": "4"
  },
  {
    "Title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2501.12948",
    "Topics": [
      "Technical Report",
      "Knowledge Distillation",
      "RLHF",
      "GRPO"
    ],
    "Description": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "Month": "2025-08",
    "Date Read": "2025-08-15",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "DeepSeek-AI",
      "Daya Guo",
      "Dejian Yang",
      "Haowei Zhang",
      "Junxiao Song",
      "Ruoyu Zhang",
      "Runxin Xu",
      "Qihao Zhu",
      "Shirong Ma",
      "Peiyi Wang",
      "Xiao Bi",
      "Xiaokang Zhang",
      "Xingkai Yu",
      "Yu Wu",
      "Z. F. Wu",
      "Zhibin Gou",
      "Zhihong Shao",
      "Zhuoshu Li",
      "Ziyi Gao",
      "Aixin Liu",
      "Bing Xue",
      "Bingxuan Wang",
      "Bochao Wu",
      "Bei Feng",
      "Chengda Lu",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Chenyu Zhang",
      "Chong Ruan",
      "Damai Dai",
      "Deli Chen",
      "Dongjie Ji",
      "Erhang Li",
      "Fangyun Lin",
      "Fucong Dai",
      "Fuli Luo",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Han Bao",
      "Hanwei Xu",
      "Haocheng Wang",
      "Honghui Ding",
      "Huajian Xin",
      "Huazuo Gao",
      "Hui Qu",
      "Hui Li",
      "Jianzhong Guo",
      "Jiashi Li",
      "Jiawei Wang",
      "Jingchang Chen",
      "Jingyang Yuan",
      "Junjie Qiu",
      "Junlong Li",
      "J. L. Cai",
      "Jiaqi Ni",
      "Jian Liang",
      "Jin Chen",
      "Kai Dong",
      "Kai Hu",
      "Kaige Gao",
      "Kang Guan",
      "Kexin Huang",
      "Kuai Yu",
      "Lean Wang",
      "Lecong Zhang",
      "Liang Zhao",
      "Litong Wang",
      "Liyue Zhang",
      "Lei Xu",
      "Leyi Xia",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Meng Li",
      "Miaojun Wang",
      "Mingming Li",
      "Ning Tian",
      "Panpan Huang",
      "Peng Zhang",
      "Qiancheng Wang",
      "Qinyu Chen",
      "Qiushi Du",
      "Ruiqi Ge",
      "Ruisong Zhang",
      "Ruizhe Pan",
      "Runji Wang",
      "R. J. Chen",
      "R. L. Jin",
      "Ruyi Chen",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shengfeng Ye",
      "Shiyu Wang",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Shuting Pan",
      "S. S. Li",
      "Shuang Zhou",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "Tao Yun",
      "Tian Pei",
      "Tianyu Sun",
      "T. Wang",
      "Wangding Zeng",
      "Wanjia Zhao",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjun Gao",
      "Wenqin Yu",
      "Wentao Zhang",
      "W. L. Xiao",
      "Wei An",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaokang Chen",
      "Xiaotao Nie",
      "Xin Cheng",
      "Xin Liu",
      "Xin Xie",
      "Xingchao Liu",
      "Xinyu Yang",
      "Xinyuan Li",
      "Xuecheng Su",
      "Xuheng Lin",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xiaojin Shen",
      "Xiaosha Chen",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Xinnan Song",
      "Xinyi Zhou",
      "Xianzu Wang",
      "Xinxia Shan",
      "Y. K. Li",
      "Y. Q. Wang",
      "Y. X. Wei",
      "Yang Zhang",
      "Yanhong Xu",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Wang",
      "Yi Yu",
      "Yichao Zhang",
      "Yifan Shi",
      "Yiliang Xiong",
      "Ying He",
      "Yishi Piao",
      "Yisong Wang",
      "Yixuan Tan",
      "Yiyang Ma",
      "Yiyuan Liu",
      "Yongqiang Guo",
      "Yuan Ou",
      "Yuduan Wang",
      "Yue Gong",
      "Yuheng Zou",
      "Yujia He",
      "Yunfan Xiong",
      "Yuxiang Luo",
      "Yuxiang You",
      "Yuxuan Liu",
      "Yuyang Zhou",
      "Y. X. Zhu",
      "Yanhong Xu",
      "Yanping Huang",
      "Yaohui Li",
      "Yi Zheng",
      "Yuchen Zhu",
      "Yunxian Ma",
      "Ying Tang",
      "Yukun Zha",
      "Yuting Yan",
      "Z. Z. Ren",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhean Xu",
      "Zhenda Xie",
      "Zhengyan Zhang",
      "Zhewen Hao",
      "Zhicheng Ma",
      "Zhigang Yan",
      "Zhiyu Wu",
      "Zihui Gu",
      "Zijia Zhu",
      "Zijun Liu",
      "Zilin Li",
      "Ziwei Xie",
      "Ziyang Song",
      "Zizheng Pan",
      "Zhen Huang",
      "Zhipeng Xu",
      "Zhongyu Zhang",
      "Zhen Zhang"
    ],
    "Date Updated": "2025-01-22",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.12948v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "DeepSeek-V3 Technical Report",
    "Reading Status": "Read",
    "Date Published": "2024-12-27",
    "Link": "https://arxiv.org/abs/2412.19437",
    "Topics": [
      "Technical Report",
      "MOE",
      "Multi-Head Latent Attention",
      "GRPO"
    ],
    "Description": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "DeepSeek-AI",
      "Aixin Liu",
      "Bei Feng",
      "Bing Xue",
      "Bingxuan Wang",
      "Bochao Wu",
      "Chengda Lu",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Chenyu Zhang",
      "Chong Ruan",
      "Damai Dai",
      "Daya Guo",
      "Dejian Yang",
      "Deli Chen",
      "Dongjie Ji",
      "Erhang Li",
      "Fangyun Lin",
      "Fucong Dai",
      "Fuli Luo",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Han Bao",
      "Hanwei Xu",
      "Haocheng Wang",
      "Haowei Zhang",
      "Honghui Ding",
      "Huajian Xin",
      "Huazuo Gao",
      "Hui Li",
      "Hui Qu",
      "J. L. Cai",
      "Jian Liang",
      "Jianzhong Guo",
      "Jiaqi Ni",
      "Jiashi Li",
      "Jiawei Wang",
      "Jin Chen",
      "Jingchang Chen",
      "Jingyang Yuan",
      "Junjie Qiu",
      "Junlong Li",
      "Junxiao Song",
      "Kai Dong",
      "Kai Hu",
      "Kaige Gao",
      "Kang Guan",
      "Kexin Huang",
      "Kuai Yu",
      "Lean Wang",
      "Lecong Zhang",
      "Lei Xu",
      "Leyi Xia",
      "Liang Zhao",
      "Litong Wang",
      "Liyue Zhang",
      "Meng Li",
      "Miaojun Wang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Mingming Li",
      "Ning Tian",
      "Panpan Huang",
      "Peiyi Wang",
      "Peng Zhang",
      "Qiancheng Wang",
      "Qihao Zhu",
      "Qinyu Chen",
      "Qiushi Du",
      "R. J. Chen",
      "R. L. Jin",
      "Ruiqi Ge",
      "Ruisong Zhang",
      "Ruizhe Pan",
      "Runji Wang",
      "Runxin Xu",
      "Ruoyu Zhang",
      "Ruyi Chen",
      "S. S. Li",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "Shengfeng Ye",
      "Shirong Ma",
      "Shiyu Wang",
      "Shuang Zhou",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Shuting Pan",
      "T. Wang",
      "Tao Yun",
      "Tian Pei",
      "Tianyu Sun",
      "W. L. Xiao",
      "Wangding Zeng",
      "Wanjia Zhao",
      "Wei An",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjun Gao",
      "Wenqin Yu",
      "Wentao Zhang",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xianzu Wang",
      "Xiao Bi",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaojin Shen",
      "Xiaokang Chen",
      "Xiaokang Zhang",
      "Xiaosha Chen",
      "Xiaotao Nie",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Xin Cheng",
      "Xin Liu",
      "Xin Xie",
      "Xingchao Liu",
      "Xingkai Yu",
      "Xinnan Song",
      "Xinxia Shan",
      "Xinyi Zhou",
      "Xinyu Yang",
      "Xinyuan Li",
      "Xuecheng Su",
      "Xuheng Lin",
      "Y. K. Li",
      "Y. Q. Wang",
      "Y. X. Wei",
      "Y. X. Zhu",
      "Yang Zhang",
      "Yanhong Xu",
      "Yanhong Xu",
      "Yanping Huang",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Li",
      "Yaohui Wang",
      "Yi Yu",
      "Yi Zheng",
      "Yichao Zhang",
      "Yifan Shi",
      "Yiliang Xiong",
      "Ying He",
      "Ying Tang",
      "Yishi Piao",
      "Yisong Wang",
      "Yixuan Tan",
      "Yiyang Ma",
      "Yiyuan Liu",
      "Yongqiang Guo",
      "Yu Wu",
      "Yuan Ou",
      "Yuchen Zhu",
      "Yuduan Wang",
      "Yue Gong",
      "Yuheng Zou",
      "Yujia He",
      "Yukun Zha",
      "Yunfan Xiong",
      "Yunxian Ma",
      "Yuting Yan",
      "Yuxiang Luo",
      "Yuxiang You",
      "Yuxuan Liu",
      "Yuyang Zhou",
      "Z. F. Wu",
      "Z. Z. Ren",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhean Xu",
      "Zhen Huang",
      "Zhen Zhang",
      "Zhenda Xie",
      "Zhengyan Zhang",
      "Zhewen Hao",
      "Zhibin Gou",
      "Zhicheng Ma",
      "Zhigang Yan",
      "Zhihong Shao",
      "Zhipeng Xu",
      "Zhiyu Wu",
      "Zhongyu Zhang",
      "Zhuoshu Li",
      "Zihui Gu",
      "Zijia Zhu",
      "Zijun Liu",
      "Zilin Li",
      "Ziwei Xie",
      "Ziyang Song",
      "Ziyi Gao",
      "Zizheng Pan"
    ],
    "Date Updated": "2025-02-18",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.19437v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
    "Reading Status": "Read",
    "Date Published": "2024-12-13",
    "Link": "https://arxiv.org/abs/2412.10302",
    "Topics": [
      "VLM",
      "MOE",
      "Multimodal",
      "Technical Report"
    ],
    "Description": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL, through two key major upgrades. For the vision component, we incorporate a dynamic tiling vision encoding strategy designed for processing high-resolution images with different aspect ratios. For the language component, we leverage DeepSeekMoE models with the Multi-head Latent Attention mechanism, which compresses Key-Value cache into latent vectors, to enable efficient inference and high throughput. Trained on an improved vision-language dataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding. Our model series is composed of three variants: DeepSeek-VL2-Tiny, DeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated parameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art performance with similar or fewer activated parameters compared to existing open-source dense and MoE-based models. Codes and pre-trained models are publicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Zhiyu Wu",
      "Xiaokang Chen",
      "Zizheng Pan",
      "Xingchao Liu",
      "Wen Liu",
      "Damai Dai",
      "Huazuo Gao",
      "Yiyang Ma",
      "Chengyue Wu",
      "Bingxuan Wang",
      "Zhenda Xie",
      "Yu Wu",
      "Kai Hu",
      "Jiawei Wang",
      "Yaofeng Sun",
      "Yukun Li",
      "Yishi Piao",
      "Kang Guan",
      "Aixin Liu",
      "Xin Xie",
      "Yuxiang You",
      "Kai Dong",
      "Xingkai Yu",
      "Haowei Zhang",
      "Liang Zhao",
      "Yisong Wang",
      "Chong Ruan"
    ],
    "Date Updated": "2024-12-13",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.10302v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "Reading Status": "Read",
    "Date Published": "2023-05-29",
    "Link": "https://arxiv.org/abs/2305.18290",
    "Topics": [
      "DPO",
      "RLHF"
    ],
    "Description": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Rafael Rafailov",
      "Archit Sharma",
      "Eric Mitchell",
      "Stefano Ermon",
      "Christopher D. Manning",
      "Chelsea Finn"
    ],
    "Date Updated": "2024-07-29",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2305.18290v3",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "Version": "3"
  },
  {
    "Title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
    "Reading Status": "Read",
    "Date Published": "2019-10-02",
    "Link": "https://arxiv.org/abs/1910.01108",
    "Topics": [
      "Knowledge Distillation",
      "BERT"
    ],
    "Description": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Victor Sanh",
      "Lysandre Debut",
      "Julien Chaumond",
      "Thomas Wolf"
    ],
    "Date Updated": "2020-03-01",
    "Comment": "February 2020 - Revision: fix bug in evaluation metrics, updated\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\n  - NeurIPS 2019",
    "PDF Link": "http://arxiv.org/pdf/1910.01108v4",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "4"
  },
  {
    "Title": "Distilling the Knowledge in a Neural Network",
    "Reading Status": "Read",
    "Date Published": "2015-03-09",
    "Link": "https://arxiv.org/abs/1503.02531",
    "Topics": [
      "Neural Network",
      "Knowledge Distillation"
    ],
    "Description": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Geoffrey Hinton",
      "Oriol Vinyals",
      "Jeff Dean"
    ],
    "Date Updated": "2015-03-09",
    "Comment": "NIPS 2014 Deep Learning Workshop",
    "PDF Link": "http://arxiv.org/pdf/1503.02531v1",
    "Primary Category": "stat.ML",
    "All Categories": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ],
    "Version": "1"
  },
  {
    "Title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking",
    "Reading Status": "Read",
    "Date Published": "2024-05-28",
    "Link": "https://arxiv.org/abs/2405.18414",
    "Topics": [
      "GraphDB",
      "Reranker",
      "RAG"
    ],
    "Description": "Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jialin Dong",
      "Bahare Fatemi",
      "Bryan Perozzi",
      "Lin F. Yang",
      "Anton Tsitsulin"
    ],
    "Date Updated": "2024-05-28",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2405.18414v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "Version": "1"
  },
  {
    "Title": "Efficient Guided Generation for Large Language Models",
    "Reading Status": "Want to Read",
    "Date Published": "2023-07-19",
    "Link": "https://arxiv.org/abs/2307.09702",
    "Topics": [],
    "Description": "In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Brandon T. Willard",
      "Rémi Louf"
    ],
    "Date Updated": "2023-08-19",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2307.09702v4",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.LG"
    ],
    "Version": "4"
  },
  {
    "Title": "EfficientLLM: Efficiency in Large Language Models",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2505.13840",
    "Topics": [
      "Quantization",
      "Fine-tuning",
      "Attention Mechanism"
    ],
    "Description": "Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.",
    "Month": "2025-08",
    "Date Read": "2025-08-18",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Zhengqing Yuan",
      "Weixiang Sun",
      "Yixin Liu",
      "Huichi Zhou",
      "Rong Zhou",
      "Yiyang Li",
      "Zheyuan Zhang",
      "Wei Song",
      "Yue Huang",
      "Haolong Jia",
      "Keerthiram Murugesan",
      "Yu Wang",
      "Lifang He",
      "Jianfeng Gao",
      "Lichao Sun",
      "Yanfang Ye"
    ],
    "Date Updated": "2025-05-20",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.13840v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-13",
    "Link": "https://arxiv.org/abs/2501.07391",
    "Topics": [],
    "Description": "Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios. Our code and implementation details are publicly available.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Siran Li",
      "Linus Stenzel",
      "Carsten Eickhoff",
      "Seyed Ali Bahrainian"
    ],
    "Date Updated": "2025-01-13",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.07391v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "Evolving Deeper LLM Thinking",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-17",
    "Link": "https://arxiv.org/abs/2501.09891",
    "Topics": [],
    "Description": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Kuang-Huei Lee",
      "Ian Fischer",
      "Yueh-Hua Wu",
      "Dave Marwood",
      "Shumeet Baluja",
      "Dale Schuurmans",
      "Xinyun Chen"
    ],
    "Date Updated": "2025-01-17",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.09891v1",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "FP4 All the Way: Fully Quantized Training of LLMs",
    "Reading Status": "Read",
    "Date Published": "2025-05-25",
    "Link": "https://arxiv.org/abs/2505.19115",
    "Topics": [
      "Quantization"
    ],
    "Description": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Brian Chmiel",
      "Maxim Fishman",
      "Ron Banner",
      "Daniel Soudry"
    ],
    "Date Updated": "2025-08-10",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.19115v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, Age",
    "Reading Status": "Read",
    "Date Published": "2019-08-14",
    "Link": "https://arxiv.org/abs/1908.04913",
    "Topics": [
      "Dataset",
      "Computer Vision"
    ],
    "Description": "Existing public face datasets are strongly biased toward Caucasian faces, and other races (e.g., Latino) are significantly underrepresented. This can lead to inconsistent model accuracy, limit the applicability of face analytic systems to non-White race groups, and adversely affect research findings based on such skewed data. To mitigate the race bias in these datasets, we construct a novel face image dataset, containing 108,501 images, with an emphasis of balanced race composition in the dataset. We define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle East, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. Evaluations were performed on existing face attribute datasets as well as novel image datasets to measure generalization performance. We find that the model trained from our dataset is substantially more accurate on novel datasets and the accuracy is consistent between race and gender groups. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Kimmo Kärkkäinen",
      "Jungseock Joo"
    ],
    "Date Updated": "2019-08-14",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/1908.04913v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Fast and Simplex: 2-Simplicial Attention in Triton",
    "Reading Status": "Read",
    "Date Published": "2025-07-03",
    "Link": "https://arxiv.org/abs/2507.02754",
    "Topics": [
      "Attention Mechanism",
      "2-Simplical Attention"
    ],
    "Description": "Recent work has shown that training loss scales as a power law with both model size and the number of tokens, and that achieving compute-optimal models requires scaling model size and token count together. However, these scaling laws assume an infinite supply of data and apply primarily in compute-bound settings. As modern large language models increasingly rely on massive internet-scale datasets, the assumption that they are compute-bound is becoming less valid. This shift highlights the need for architectures that prioritize token efficiency.   In this work, we investigate the use of the 2-simplicial Transformer, an architecture that generalizes standard dot-product attention to trilinear functions through an efficient Triton kernel implementation. We demonstrate that the 2-simplicial Transformer achieves better token efficiency than standard Transformers: for a fixed token budget, similarly sized models outperform their dot-product counterparts on tasks involving mathematics, coding, reasoning, and logic. We quantify these gains by demonstrating that $2$-simplicial attention changes the exponent in the scaling laws for knowledge and reasoning tasks compared to dot product attention.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Aurko Roy",
      "Timothy Chou",
      "Sai Surya Duvvuri",
      "Sijia Chen",
      "Jiecao Yu",
      "Xiaodong Wang",
      "Manzil Zaheer",
      "Rohan Anil"
    ],
    "Date Updated": "2025-07-03",
    "Comment": "10 pages, with appendix 25 pages",
    "PDF Link": "http://arxiv.org/pdf/2507.02754v1",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "Reading Status": "Read",
    "Date Published": "2023-07-17",
    "Link": "https://arxiv.org/abs/2307.08691",
    "Topics": [
      "Flash Attention",
      "Attention Mechanism"
    ],
    "Description": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tri Dao"
    ],
    "Date Updated": "2023-07-17",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2307.08691v1",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
    "Reading Status": "Read",
    "Date Published": "2024-07-11",
    "Link": "https://arxiv.org/abs/2407.08608",
    "Topics": [
      "Flash Attention",
      "Attention Mechanism"
    ],
    "Description": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jay Shah",
      "Ganesh Bikshandi",
      "Ying Zhang",
      "Vijay Thakkar",
      "Pradeep Ramani",
      "Tri Dao"
    ],
    "Date Updated": "2024-07-12",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2407.08608v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "Reading Status": "Read",
    "Date Published": "2022-05-27",
    "Link": "https://arxiv.org/abs/2205.14135",
    "Topics": [
      "Flash Attention",
      "Attention Mechanism"
    ],
    "Description": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tri Dao",
      "Daniel Y. Fu",
      "Stefano Ermon",
      "Atri Rudra",
      "Christopher Ré"
    ],
    "Date Updated": "2022-06-23",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2205.14135v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
    "Reading Status": "Read",
    "Date Published": "2024-08-20",
    "Link": "https://arxiv.org/abs/2408.10774",
    "Topics": [
      "LoRA",
      "Fine-tuning"
    ],
    "Description": "Large Language Models (LLMs) are driving advancements in artificial intelligence by increasing the scale of model parameters, which has significantly enhanced generalization ability and unlocked new capabilities in practice. However, their performance in specific downstream tasks is usually hindered by their knowledge boundaries on these tasks. Thus, fine-tuning techniques, especially the widely used Low-Rank Adaptation (LoRA) method, have been introduced to expand the boundaries on these tasks, whereas LoRA would underperform on certain tasks owing to its potential overfitting on these tasks. To overcome this overfitting and improve the performance of LoRA, we propose the flexible low rank adaptation (Flexora) method to automatically and flexibly select the most important layers needing to be fine-tuned to achieve the best performance on different downstream tasks. Specifically, Flexora firstly frames this layer selection problem as a well-defined hyperparameter optimization (HPO) problem, then addresses it using the unrolled differentiation (UD) method, and finally selects the most useful layers based on the optimized hyperparameters. Our extensive experiments on many pretrained models and natural language tasks show that Flexora is able to consistently improve over the existing baselines, indicating the effectiveness of our Flexora in practice. We additionally provide insightful theoretical results and many ablation studies to deliver a comprehensive understanding of our Flexora. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Chenxing Wei",
      "Yao Shu",
      "Ying Tiffany He",
      "Fei Richard Yu"
    ],
    "Date Updated": "2025-07-01",
    "Comment": "40 pages, 15 figures",
    "PDF Link": "http://arxiv.org/pdf/2408.10774v4",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI",
      "cs.CL"
    ],
    "Version": "4"
  },
  {
    "Title": "Foundations of Large Language Models",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-16",
    "Link": "https://arxiv.org/abs/2501.09223",
    "Topics": [],
    "Description": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "Date Updated": "2025-06-15",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.09223v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
    "Reading Status": "Read",
    "Date Published": "2025-06-17",
    "Link": "https://arxiv.org/abs/2506.14761",
    "Topics": [
      "U-Nets"
    ],
    "Description": "Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Mathurin Videau",
      "Badr Youbi Idrissi",
      "Alessandro Leite",
      "Marc Schoenauer",
      "Olivier Teytaud",
      "David Lopez-Paz"
    ],
    "Date Updated": "2025-06-17",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2506.14761v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "GEAR: Graph-enhanced Agent for Retrieval-augmented Generation",
    "Reading Status": "Want to Read",
    "Date Published": "2024-12-24",
    "Link": "https://arxiv.org/abs/2412.18431",
    "Topics": [
      "GraphDB",
      "RAG"
    ],
    "Description": "Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GeAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GeAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Zhili Shen",
      "Chenxin Diao",
      "Pavlos Vougiouklis",
      "Pascual Merita",
      "Shriram Piramanayagam",
      "Enting Chen",
      "Damien Graux",
      "Andre Melo",
      "Ruofei Lai",
      "Zeren Jiang",
      "Zhongyang Li",
      "YE QI",
      "Yang Ren",
      "Dandan Tu",
      "Jeff Z. Pan"
    ],
    "Date Updated": "2025-06-22",
    "Comment": "ACL 2025 Findings",
    "PDF Link": "http://arxiv.org/pdf/2412.18431v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "Version": "2"
  },
  {
    "Title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
    "Reading Status": "Read",
    "Date Published": "2025-05-10",
    "Link": "https://arxiv.org/abs/2505.06708",
    "Topics": [
      "Gated-Delta Attention",
      "Attention Mechanism"
    ],
    "Description": "Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification-applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)-consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates 'attention sink' and enhances long-context extrapolation performance, and we also release related $\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and $\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate future research.",
    "Month": "2025-12",
    "Date Read": "2025-12-28",
    "Date Added": "2025-12-28",
    "WeekDay": "Sunday",
    "Week": 52,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Gated Delta Networks: Improving Mamba2 with Delta Rule",
    "Reading Status": "Read",
    "Date Published": "2024-12-09",
    "Link": "https://arxiv.org/abs/2412.06464",
    "Topics": [
      "Gated-Delta Attention",
      "Attention Mechanism"
    ],
    "Description": "Linear Transformers have gained attention as efficient alternatives to standard Transformers, but their performance in retrieval and long-context tasks has been limited. To address these limitations, recent work has explored two distinct mechanisms: gating for adaptive memory control and the delta update rule for precise memory modifications. We observe that these mechanisms are complementary: gating enables rapid memory erasure while the delta rule facilitates targeted updates. Building on this insight, we introduce the gated delta rule and develop a parallel training algorithm optimized for modern hardware. Our proposed architecture, Gated DeltaNet, consistently surpasses existing models like Mamba2 and DeltaNet across multiple benchmarks, including language modeling, common-sense reasoning, in-context retrieval, length extrapolation, and long-context understanding. We further enhance performance by developing hybrid architectures that combine Gated DeltaNet layers with sliding window attention or Mamba2 layers, achieving both improved training efficiency and superior task performance.",
    "Month": "2025-12",
    "Date Read": "2026-01-15",
    "Date Added": "2025-12-28",
    "WeekDay": "Sunday",
    "Week": 52,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "Reading Status": "Read",
    "Date Published": "2025-03-10",
    "Link": "https://www.arxiv.org/abs/2503.07891",
    "Topics": [
      "Embeddings"
    ],
    "Description": "In this report, we introduce Gemini Embedding, a state-of-the-art embedding model leveraging the power of Gemini, Google's most capable large language model. Capitalizing on Gemini's inherent multilingual and code understanding capabilities, Gemini Embedding produces highly generalizable embeddings for text spanning numerous languages and textual modalities. The representations generated by Gemini Embedding can be precomputed and applied to a variety of downstream tasks including classification, similarity, clustering, ranking, and retrieval. Evaluated on the Massive Multilingual Text Embedding Benchmark (MMTEB), which includes over one hundred tasks across 250+ languages, Gemini Embedding substantially outperforms prior state-of-the-art models, demonstrating considerable improvements in embedding quality. Achieving state-of-the-art performance across MMTEB's multilingual, English, and code benchmarks, our unified model demonstrates strong capabilities across a broad selection of tasks and surpasses specialized domain-specific models.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jinhyuk Lee",
      "Feiyang Chen",
      "Sahil Dua",
      "Daniel Cer",
      "Madhuri Shanbhogue",
      "Iftekhar Naim",
      "Gustavo Hernández Ábrego",
      "Zhe Li",
      "Kaifeng Chen",
      "Henrique Schechter Vera",
      "Xiaoqi Ren",
      "Shanfeng Zhang",
      "Daniel Salz",
      "Michael Boratko",
      "Jay Han",
      "Blair Chen",
      "Shuo Huang",
      "Vikram Rao",
      "Paul Suganthan",
      "Feng Han",
      "Andreas Doumanoglou",
      "Nithi Gupta",
      "Fedor Moiseev",
      "Cathy Yip",
      "Aashi Jain",
      "Simon Baumgartner",
      "Shahrokh Shahi",
      "Frank Palma Gomez",
      "Sandeep Mariserla",
      "Min Choi",
      "Parashar Shah",
      "Sonam Goenka",
      "Ke Chen",
      "Ye Xia",
      "Koert Chen",
      "Sai Meher Karthik Duddu",
      "Yichang Chen",
      "Trevor Walker",
      "Wenlei Zhou",
      "Rakesh Ghiya",
      "Zach Gleicher",
      "Karan Gill",
      "Zhe Dong",
      "Mojtaba Seyedhosseini",
      "Yunhsuan Sung",
      "Raphael Hoffmann",
      "Tom Duerig"
    ],
    "Date Updated": "2025-03-10",
    "Comment": "19 pages",
    "PDF Link": "http://arxiv.org/pdf/2503.07891v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "Gemma 3 Technical Report",
    "Reading Status": "Read",
    "Date Published": "",
    "Link": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
    "Topics": [
      "Technical Report"
    ],
    "Description": "We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging\nin scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider\ncoverage of languages and longer context – at least 128K tokens. We also change the architecture of\nthe model to reduce the KV-cache memory that tends to explode with long context. This is achieved by\nincreasing the ratio of local to global attention layers, and keeping the span on local attention short.\nThe Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2\nfor both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe\nsignificantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-\n4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across\nbenchmarks. We release all our models to the community.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [],
    "Date Updated": null,
    "Comment": NaN,
    "PDF Link": NaN,
    "Primary Category": NaN,
    "All Categories": [],
    "Version": NaN
  },
  {
    "Title": "Granite-speech: open-source speech-aware LLMs with strong English ASR\n  capabilities",
    "Reading Status": "Reading",
    "Date Published": "2025-05-13",
    "Link": "https://arxiv.org/abs/2505.08699",
    "Topics": [
      "Audio"
    ],
    "Description": "Granite-speech LLMs are compact and efficient speech language models specifically designed for English ASR and automatic speech translation (AST). The models were trained by modality aligning the 2B and 8B parameter variants of granite-3.3-instruct to speech on publicly available open-source corpora containing audio inputs and text targets consisting of either human transcripts for ASR or automatically generated translations for AST. Comprehensive benchmarking shows that on English ASR, which was our primary focus, they outperform several competitors' models that were trained on orders of magnitude more proprietary data, and they keep pace on English-to-X AST for major European languages, Japanese, and Chinese. The speech-specific components are: a conformer acoustic encoder using block attention and self-conditioning trained with connectionist temporal classification, a windowed query-transformer speech modality adapter used to do temporal downsampling of the acoustic embeddings and map them to the LLM text embedding space, and LoRA adapters to further fine-tune the text LLM. Granite-speech-3.3 operates in two modes: in speech mode, it performs ASR and AST by activating the encoder, projector, and LoRA adapters; in text mode, it calls the underlying granite-3.3-instruct model directly (without LoRA), essentially preserving all the text LLM capabilities and safety. Both models are freely available on HuggingFace (https://huggingface.co/ibm-granite/granite-speech-3.3-2b and https://huggingface.co/ibm-granite/granite-speech-3.3-8b) and can be used for both research and commercial purposes under a permissive Apache 2.0 license.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-30",
    "WeekDay": "Saturday",
    "Week": 35,
    "Authors": [
      "George Saon",
      "Avihu Dekel",
      "Alexander Brooks",
      "Tohru Nagano",
      "Abraham Daniels",
      "Aharon Satt",
      "Ashish Mittal",
      "Brian Kingsbury",
      "David Haws",
      "Edmilson Morais",
      "Gakuto Kurata",
      "Hagai Aronowitz",
      "Ibrahim Ibrahim",
      "Jeff Kuo",
      "Kate Soule",
      "Luis Lastras",
      "Masayuki Suzuki",
      "Ron Hoory",
      "Samuel Thomas",
      "Sashi Novitasari",
      "Takashi Fukuda",
      "Vishal Sunder",
      "Xiaodong Cui",
      "Zvi Kons"
    ],
    "Date Updated": "2025-05-14",
    "Comment": "7 pages, 9 figures",
    "PDF Link": "http://arxiv.org/pdf/2505.08699v2",
    "Primary Category": "eess.AS",
    "All Categories": [
      "eess.AS"
    ],
    "Version": "2"
  },
  {
    "Title": "Group Sequence Policy Optimization",
    "Reading Status": "Reading",
    "Date Published": "2025-07-24",
    "Link": "https://arxiv.org/abs/2507.18071",
    "Topics": [
      "RLHF",
      "GSPO",
      "PPO",
      "GRPO"
    ],
    "Description": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Chujie Zheng",
      "Shixuan Liu",
      "Mingze Li",
      "Xiong-Hui Chen",
      "Bowen Yu",
      "Chang Gao",
      "Kai Dang",
      "Yuqiong Liu",
      "Rui Men",
      "An Yang",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "Date Updated": "2025-07-28",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2507.18071v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "Hierarchical Reasoning Model",
    "Reading Status": "Read",
    "Date Published": "2025-06-26",
    "Link": "https://arxiv.org/abs/2506.21734",
    "Topics": [
      "HRM"
    ],
    "Description": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.",
    "Month": "2025-11",
    "Date Read": "2025-11-07",
    "Date Added": "2025-11-07",
    "WeekDay": "Friday",
    "Week": 45,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Image-to-Image Translation with Condotional Adversarial Networks",
    "Reading Status": "Read",
    "Date Published": "2016-11-21",
    "Link": "https://arxiv.org/abs/1611.07004",
    "Topics": [
      "Computer Vision",
      "GAN"
    ],
    "Description": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Phillip Isola",
      "Jun-Yan Zhu",
      "Tinghui Zhou",
      "Alexei A. Efros"
    ],
    "Date Updated": "2018-11-26",
    "Comment": "Website: https://phillipi.github.io/pix2pix/, CVPR 2017",
    "PDF Link": "http://arxiv.org/pdf/1611.07004v3",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV"
    ],
    "Version": "3"
  },
  {
    "Title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-13",
    "Link": "https://arxiv.org/abs/2501.07542",
    "Topics": [],
    "Description": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Chengzu Li",
      "Wenshan Wu",
      "Huanyu Zhang",
      "Yan Xia",
      "Shaoguang Mao",
      "Li Dong",
      "Ivan Vulić",
      "Furu Wei"
    ],
    "Date Updated": "2025-01-13",
    "Comment": "11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)",
    "PDF Link": "http://arxiv.org/pdf/2501.07542v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
    "Reading Status": "Read",
    "Date Published": "2025-01-25",
    "Link": "https://arxiv.org/abs/2501.15228",
    "Topics": [
      "RAG"
    ],
    "Description": "Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents' goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yiqun Chen",
      "Lingyong Yan",
      "Weiwei Sun",
      "Xinyu Ma",
      "Yi Zhang",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Yiming Yang",
      "Jiaxin Mao"
    ],
    "Date Updated": "2025-01-25",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2501.15228v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.IR"
    ],
    "Version": "1"
  },
  {
    "Title": "Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling",
    "Reading Status": "Read",
    "Date Published": "2025-01-29",
    "Link": "https://arxiv.org/abs/2501.17811",
    "Topics": [
      "Computer Vision",
      "VLM",
      "Technical Report"
    ],
    "Description": "In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Xiaokang Chen",
      "Zhiyu Wu",
      "Xingchao Liu",
      "Zizheng Pan",
      "Wen Liu",
      "Zhenda Xie",
      "Xingkai Yu",
      "Chong Ruan"
    ],
    "Date Updated": "2025-01-29",
    "Comment": "Research paper. arXiv admin note: text overlap with arXiv:2410.13848",
    "PDF Link": "http://arxiv.org/pdf/2501.17811v1",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "Version": "1"
  },
  {
    "Title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture\n  Search",
    "Reading Status": "Read",
    "Date Published": "2025-08-21",
    "Link": "https://arxiv.org/abs/2508.15884",
    "Topics": [
      "Attention Mechanism",
      "PostNAS",
      "Nvidia",
      "Gated-Delta Attention"
    ],
    "Description": "We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.",
    "Month": "2025-09",
    "Date Read": "2026-01-03",
    "Date Added": "2025-09-07",
    "WeekDay": "Sunday",
    "Week": 36,
    "Authors": [
      "Yuxian Gu",
      "Qinghao Hu",
      "Shang Yang",
      "Haocheng Xi",
      "Junyu Chen",
      "Song Han",
      "Han Cai"
    ],
    "Date Updated": "2025-09-08",
    "Comment": "Tech Report",
    "PDF Link": "http://arxiv.org/pdf/2508.15884v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation",
    "Reading Status": "Want to Read",
    "Date Published": "2024-09-10",
    "Link": "https://arxiv.org/abs/2409.13731",
    "Topics": [],
    "Description": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Lei Liang",
      "Mengshu Sun",
      "Zhengke Gui",
      "Zhongshu Zhu",
      "Zhouyu Jiang",
      "Ling Zhong",
      "Yuan Qu",
      "Peilong Zhao",
      "Zhongpu Bo",
      "Jin Yang",
      "Huaidong Xiong",
      "Lin Yuan",
      "Jun Xu",
      "Zaoyang Wang",
      "Zhiqiang Zhang",
      "Wen Zhang",
      "Huajun Chen",
      "Wenguang Chen",
      "Jun Zhou"
    ],
    "Date Updated": "2024-09-26",
    "Comment": "33 pages",
    "PDF Link": "http://arxiv.org/pdf/2409.13731v3",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "3"
  },
  {
    "Title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
    "Reading Status": "Read",
    "Date Published": "2025-01-22",
    "Link": "https://www.arxiv.org/abs/2501.12599",
    "Topics": [
      "Technical Report",
      "RLHF"
    ],
    "Description": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Kimi Team",
      "Angang Du",
      "Bofei Gao",
      "Bowei Xing",
      "Changjiu Jiang",
      "Cheng Chen",
      "Cheng Li",
      "Chenjun Xiao",
      "Chenzhuang Du",
      "Chonghua Liao",
      "Chuning Tang",
      "Congcong Wang",
      "Dehao Zhang",
      "Enming Yuan",
      "Enzhe Lu",
      "Fengxiang Tang",
      "Flood Sung",
      "Guangda Wei",
      "Guokun Lai",
      "Haiqing Guo",
      "Han Zhu",
      "Hao Ding",
      "Hao Hu",
      "Hao Yang",
      "Hao Zhang",
      "Haotian Yao",
      "Haotian Zhao",
      "Haoyu Lu",
      "Haoze Li",
      "Haozhen Yu",
      "Hongcheng Gao",
      "Huabin Zheng",
      "Huan Yuan",
      "Jia Chen",
      "Jianhang Guo",
      "Jianlin Su",
      "Jianzhou Wang",
      "Jie Zhao",
      "Jin Zhang",
      "Jingyuan Liu",
      "Junjie Yan",
      "Junyan Wu",
      "Lidong Shi",
      "Ling Ye",
      "Longhui Yu",
      "Mengnan Dong",
      "Neo Zhang",
      "Ningchen Ma",
      "Qiwei Pan",
      "Qucheng Gong",
      "Shaowei Liu",
      "Shengling Ma",
      "Shupeng Wei",
      "Sihan Cao",
      "Siying Huang",
      "Tao Jiang",
      "Weihao Gao",
      "Weimin Xiong",
      "Weiran He",
      "Weixiao Huang",
      "Weixin Xu",
      "Wenhao Wu",
      "Wenyang He",
      "Xianghui Wei",
      "Xianqing Jia",
      "Xingzhe Wu",
      "Xinran Xu",
      "Xinxing Zu",
      "Xinyu Zhou",
      "Xuehai Pan",
      "Y. Charles",
      "Yang Li",
      "Yangyang Hu",
      "Yangyang Liu",
      "Yanru Chen",
      "Yejie Wang",
      "Yibo Liu",
      "Yidao Qin",
      "Yifeng Liu",
      "Ying Yang",
      "Yiping Bao",
      "Yulun Du",
      "Yuxin Wu",
      "Yuzhi Wang",
      "Zaida Zhou",
      "Zhaoji Wang",
      "Zhaowei Li",
      "Zhen Zhu",
      "Zheng Zhang",
      "Zhexu Wang",
      "Zhilin Yang",
      "Zhiqi Huang",
      "Zihao Huang",
      "Ziyao Xu",
      "Zonghan Yang",
      "Zongyu Lin"
    ],
    "Date Updated": "2025-06-03",
    "Comment": "25 pages",
    "PDF Link": "http://arxiv.org/pdf/2501.12599v4",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI",
      "cs.LG"
    ],
    "Version": "4"
  },
  {
    "Title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
    "Reading Status": "Want to Read",
    "Date Published": "2025-03-06",
    "Link": "https://arxiv.org/abs/2503.04697",
    "Topics": [],
    "Description": "Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Pranjal Aggarwal",
      "Sean Welleck"
    ],
    "Date Updated": "2025-03-06",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2503.04697v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!",
    "Reading Status": "Read",
    "Date Published": "2025-02-11",
    "Link": "https://arxiv.org/abs/2502.07374",
    "Topics": [
      "Fine-tuning",
      "Knowledge Distillation"
    ],
    "Description": "Large reasoning models (LRMs) tackle complex reasoning problems by following long chain-of-thoughts (Long CoT) that incorporate reflection, backtracking, and self-validation. However, the training techniques and data requirements to elicit Long CoT remain poorly understood. In this work, we find that a Large Language model (LLM) can effectively learn Long CoT reasoning through data-efficient supervised fine-tuning (SFT) and parameter-efficient low-rank adaptation (LoRA). With just 17k long CoT training samples, the Qwen2.5-32B-Instruct model achieves significant improvements on a wide range of math and coding benchmarks, including 56.7% (+40.0%) on AIME 2024 and 57.0% (+8.1%) on LiveCodeBench, competitive to the proprietary o1-preview model's score of 44.6% and 59.1%. More importantly, we find that the structure of Long CoT is critical to the learning process, whereas the content of individual reasoning steps has minimal impact. Perturbations affecting content, such as training on incorrect samples or removing reasoning keywords, have little impact on performance. In contrast, structural modifications that disrupt logical consistency in the Long CoT, such as shuffling or deleting reasoning steps, significantly degrade accuracy. For example, a model trained on Long CoT samples with incorrect answers still achieves only 3.2% lower accuracy compared to training with fully correct samples. These insights deepen our understanding of how to elicit reasoning capabilities in LLMs and highlight key considerations for efficiently training the next generation of reasoning models. This is the academic paper of our previous released Sky-T1-32B-Preview model. Codes are available at https://github.com/NovaSky-AI/SkyThought.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Dacheng Li",
      "Shiyi Cao",
      "Tyler Griggs",
      "Shu Liu",
      "Xiangxi Mo",
      "Eric Tang",
      "Sumanth Hegde",
      "Kourosh Hakhamaneshi",
      "Shishir G. Patil",
      "Matei Zaharia",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "Date Updated": "2025-02-18",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2502.07374v2",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "Large Language Diffusion Models",
    "Reading Status": "Read",
    "Date Published": "2025-02-14",
    "Link": "https://arxiv.org/abs/2502.09992",
    "Topics": [
      "Diffusion Models"
    ],
    "Description": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong scalability and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.",
    "Month": "2025-11",
    "Date Read": "2025-12-06",
    "Date Added": "2025-11-07",
    "WeekDay": "Friday",
    "Week": 45,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Learning Transferable Visual Models From Natural Language Supervision",
    "Reading Status": "Read",
    "Date Published": "2021-02-26",
    "Link": "https://arxiv.org/abs/2103.00020",
    "Topics": [
      "Computer Vision",
      "CLIP"
    ],
    "Description": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at this https URL. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Alec Radford",
      "Jong Wook Kim",
      "Chris Hallacy",
      "Aditya Ramesh",
      "Gabriel Goh",
      "Sandhini Agarwal",
      "Girish Sastry",
      "Amanda Askell",
      "Pamela Mishkin",
      "Jack Clark",
      "Gretchen Krueger",
      "Ilya Sutskever"
    ],
    "Date Updated": "2021-02-26",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2103.00020v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Less is More: Recursive Reasoning with Tiny Networks",
    "Reading Status": "Read",
    "Date Published": "2025-10-06",
    "Link": "https://arxiv.org/abs/2510.04871",
    "Topics": [
      "HRM",
      "TRM"
    ],
    "Description": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
    "Month": "2025-11",
    "Date Read": "2025-11-09",
    "Date Added": "2025-11-07",
    "WeekDay": "Friday",
    "Week": 45,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "LightGlue: Local Feature Matching at Light Speed",
    "Reading Status": "Read",
    "Date Published": "2023-06-23",
    "Link": "https://arxiv.org/abs/2306.13643",
    "Topics": [
      "Flow Matching"
    ],
    "Description": "We introduce LightGlue, a deep neural network that learns to match local features across images. We revisit multiple design decisions of SuperGlue, the state of the art in sparse matching, and derive simple but effective improvements. Cumulatively, they make LightGlue more efficient - in terms of both memory and computation, more accurate, and much easier to train. One key property is that LightGlue is adaptive to the difficulty of the problem: the inference is much faster on image pairs that are intuitively easy to match, for example because of a larger visual overlap or limited appearance change. This opens up exciting prospects for deploying deep matchers in latency-sensitive applications like 3D reconstruction. The code and trained models are publicly available at https://github.com/cvg/LightGlue.",
    "Month": "2025-11",
    "Date Read": "2025-12-04",
    "Date Added": "2025-11-09",
    "WeekDay": "Sunday",
    "Week": 45,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Llama-Nemotron: Efficient Reasoning Models",
    "Reading Status": "Want to Read",
    "Date Published": "2025-05-02",
    "Link": "https://arxiv.org/abs/2505.00949",
    "Topics": [
      "Technical Report"
    ],
    "Description": "We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Akhiad Bercovich",
      "Itay Levy",
      "Izik Golan",
      "Mohammad Dabbah",
      "Ran El-Yaniv",
      "Omri Puny",
      "Ido Galil",
      "Zach Moshe",
      "Tomer Ronen",
      "Najeeb Nabwani",
      "Ido Shahaf",
      "Oren Tropp",
      "Ehud Karpas",
      "Ran Zilberstein",
      "Jiaqi Zeng",
      "Soumye Singhal",
      "Alexander Bukharin",
      "Yian Zhang",
      "Tugrul Konuk",
      "Gerald Shen",
      "Ameya Sunil Mahabaleshwarkar",
      "Bilal Kartal",
      "Yoshi Suhara",
      "Olivier Delalleau",
      "Zijia Chen",
      "Zhilin Wang",
      "David Mosallanezhad",
      "Adi Renduchintala",
      "Haifeng Qian",
      "Dima Rekesh",
      "Fei Jia",
      "Somshubra Majumdar",
      "Vahid Noroozi",
      "Wasi Uddin Ahmad",
      "Sean Narenthiran",
      "Aleksander Ficek",
      "Mehrzad Samadi",
      "Jocelyn Huang",
      "Siddhartha Jain",
      "Igor Gitman",
      "Ivan Moshkov",
      "Wei Du",
      "Shubham Toshniwal",
      "George Armstrong",
      "Branislav Kisacanin",
      "Matvei Novikov",
      "Daria Gitman",
      "Evelina Bakhturina",
      "Prasoon Varshney",
      "Makesh Narsimhan",
      "Jane Polak Scowcroft",
      "John Kamalu",
      "Dan Su",
      "Kezhi Kong",
      "Markus Kliegl",
      "Rabeeh Karimi Mahabadi",
      "Ying Lin",
      "Sanjeev Satheesh",
      "Jupinder Parmar",
      "Pritam Gundecha",
      "Brandon Norick",
      "Joseph Jennings",
      "Shrimai Prabhumoye",
      "Syeda Nahida Akter",
      "Mostofa Patwary",
      "Abhinav Khattar",
      "Deepak Narayanan",
      "Roger Waleffe",
      "Jimmy Zhang",
      "Bor-Yiing Su",
      "Guyue Huang",
      "Terry Kong",
      "Parth Chadha",
      "Sahil Jain",
      "Christine Harvey",
      "Elad Segal",
      "Jining Huang",
      "Sergey Kashirsky",
      "Robert McQueen",
      "Izzy Putterman",
      "George Lam",
      "Arun Venkatesan",
      "Sherry Wu",
      "Vinh Nguyen",
      "Manoj Kilaru",
      "Andrew Wang",
      "Anna Warno",
      "Abhilash Somasamudramath",
      "Sandip Bhaskar",
      "Maka Dong",
      "Nave Assaf",
      "Shahar Mor",
      "Omer Ullman Argov",
      "Scot Junkin",
      "Oleksandr Romanenko",
      "Pedro Larroy",
      "Monika Katariya",
      "Marco Rovinelli",
      "Viji Balas",
      "Nicholas Edelman",
      "Anahita Bhiwandiwalla",
      "Muthu Subramaniam",
      "Smita Ithape",
      "Karthik Ramamoorthy",
      "Yuting Wu",
      "Suguna Varshini Velury",
      "Omri Almog",
      "Joyjit Daw",
      "Denys Fridman",
      "Erick Galinkin",
      "Michael Evans",
      "Shaona Ghosh",
      "Katherine Luna",
      "Leon Derczynski",
      "Nikki Pope",
      "Eileen Long",
      "Seth Schneider",
      "Guillermo Siman",
      "Tomasz Grzegorzek",
      "Pablo Ribalta",
      "Monika Katariya",
      "Chris Alexiuk",
      "Joey Conway",
      "Trisha Saar",
      "Ann Guan",
      "Krzysztof Pawelec",
      "Shyamala Prayaga",
      "Oleksii Kuchaiev",
      "Boris Ginsburg",
      "Oluwatobi Olabiyi",
      "Kari Briski",
      "Jonathan Cohen",
      "Bryan Catanzaro",
      "Jonah Alben",
      "Yonatan Geifman",
      "Eric Chung"
    ],
    "Date Updated": "2025-09-09",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.00949v5",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "5"
  },
  {
    "Title": "LoRA: Low-Rank Adaptation of Large Language Models",
    "Reading Status": "Read",
    "Date Published": "2021-06-17",
    "Link": "https://arxiv.org/abs/2106.09685",
    "Topics": [
      "Fine-tuning",
      "LoRA"
    ],
    "Description": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Edward J. Hu",
      "Yelong Shen",
      "Phillip Wallis",
      "Zeyuan Allen-Zhu",
      "Yuanzhi Li",
      "Shean Wang",
      "Lu Wang",
      "Weizhu Chen"
    ],
    "Date Updated": "2021-10-16",
    "Comment": "Draft V2 includes better baselines, experiments on GLUE, and more on\n  adapter latency",
    "PDF Link": "http://arxiv.org/pdf/2106.09685v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "Reading Status": "Read",
    "Date Published": "2023-12-01",
    "Link": "https://arxiv.org/abs/2312.00752",
    "Topics": [
      "Mamba",
      "SSM",
      "Structure SSM",
      "S4",
      "S6",
      "Gated-Delta Attention"
    ],
    "Description": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
    "Month": "2025-10",
    "Date Read": "2025-10-12",
    "Date Added": "2025-10-04",
    "WeekDay": "Saturday",
    "Week": 40,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2407.19594",
    "Topics": [
      "Mixture of Judges",
      "Reward Hacking"
    ],
    "Description": "Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision. ",
    "Month": "2025-08",
    "Date Read": "2025-08-18",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tianhao Wu",
      "Weizhe Yuan",
      "Olga Golovneva",
      "Jing Xu",
      "Yuandong Tian",
      "Jiantao Jiao",
      "Jason Weston",
      "Sainbayar Sukhbaatar"
    ],
    "Date Updated": "2024-07-30",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2407.19594v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-14",
    "Link": "https://arxiv.org/abs/2501.08313",
    "Topics": [],
    "Description": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "MiniMax",
      "Aonian Li",
      "Bangwei Gong",
      "Bo Yang",
      "Boji Shan",
      "Chang Liu",
      "Cheng Zhu",
      "Chunhao Zhang",
      "Congchao Guo",
      "Da Chen",
      "Dong Li",
      "Enwei Jiao",
      "Gengxin Li",
      "Guojun Zhang",
      "Haohai Sun",
      "Houze Dong",
      "Jiadai Zhu",
      "Jiaqi Zhuang",
      "Jiayuan Song",
      "Jin Zhu",
      "Jingtao Han",
      "Jingyang Li",
      "Junbin Xie",
      "Junhao Xu",
      "Junjie Yan",
      "Kaishun Zhang",
      "Kecheng Xiao",
      "Kexi Kang",
      "Le Han",
      "Leyang Wang",
      "Lianfei Yu",
      "Liheng Feng",
      "Lin Zheng",
      "Linbo Chai",
      "Long Xing",
      "Meizhi Ju",
      "Mingyuan Chi",
      "Mozhi Zhang",
      "Peikai Huang",
      "Pengcheng Niu",
      "Pengfei Li",
      "Pengyu Zhao",
      "Qi Yang",
      "Qidi Xu",
      "Qiexiang Wang",
      "Qin Wang",
      "Qiuhui Li",
      "Ruitao Leng",
      "Shengmin Shi",
      "Shuqi Yu",
      "Sichen Li",
      "Songquan Zhu",
      "Tao Huang",
      "Tianrun Liang",
      "Weigao Sun",
      "Weixuan Sun",
      "Weiyu Cheng",
      "Wenkai Li",
      "Xiangjun Song",
      "Xiao Su",
      "Xiaodong Han",
      "Xinjie Zhang",
      "Xinzhu Hou",
      "Xu Min",
      "Xun Zou",
      "Xuyang Shen",
      "Yan Gong",
      "Yingjie Zhu",
      "Yipeng Zhou",
      "Yiran Zhong",
      "Yongyi Hu",
      "Yuanxiang Fan",
      "Yue Yu",
      "Yufeng Yang",
      "Yuhao Li",
      "Yunan Huang",
      "Yunji Li",
      "Yunpeng Huang",
      "Yunzhi Xu",
      "Yuxin Mao",
      "Zehan Li",
      "Zekang Li",
      "Zewei Tao",
      "Zewen Ying",
      "Zhaoyang Cong",
      "Zhen Qin",
      "Zhenhua Fan",
      "Zhihang Yu",
      "Zhuo Jiang",
      "Zijia Wu"
    ],
    "Date Updated": "2025-01-14",
    "Comment": "A technical report from MiniMax. The authors are listed in\n  alphabetical order. We open-sourced our MiniMax-01 at\n  https://github.com/MiniMax-AI",
    "PDF Link": "http://arxiv.org/pdf/2501.08313v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.CV"
    ],
    "Version": "1"
  },
  {
    "Title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
    "Reading Status": "Read",
    "Date Published": "2025-12-24",
    "Link": "https://arxiv.org/abs/2512.20856",
    "Topics": [
      "Nvidia",
      "Mamba",
      "MOE"
    ],
    "Description": "We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.",
    "Month": "2026-01",
    "Date Read": "2026-01-03",
    "Date Added": "2026-01-03",
    "WeekDay": "Saturday",
    "Week": 1,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse\n  Attention",
    "Reading Status": "Read",
    "Date Published": "2025-02-16",
    "Link": "https://arxiv.org/abs/2502.11089",
    "Topics": [
      "Attention Mechanism",
      "NSA"
    ],
    "Description": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jingyang Yuan",
      "Huazuo Gao",
      "Damai Dai",
      "Junyu Luo",
      "Liang Zhao",
      "Zhengyan Zhang",
      "Zhenda Xie",
      "Y. X. Wei",
      "Lean Wang",
      "Zhiping Xiao",
      "Yuqing Wang",
      "Chong Ruan",
      "Ming Zhang",
      "Wenfeng Liang",
      "Wangding Zeng"
    ],
    "Date Updated": "2025-02-27",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2502.11089v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General\n  Reasoning Tasks",
    "Reading Status": "Read",
    "Date Published": "2025-07-02",
    "Link": "https://arxiv.org/abs/2507.01921",
    "Topics": [
      "Knowledge Distillation",
      "Fine-tuning",
      "RLHF"
    ],
    "Description": "Recent work has shown that distilling reasoning traces from a larger teacher model via supervised finetuning outperforms reinforcement learning with the smaller student model alone (Guo et al. 2025). However, there has not been a systematic study of what kind of reasoning demonstrations from the teacher are most effective in improving the student model's reasoning capabilities. In this work we curate high-quality \"NaturalThoughts\" by selecting reasoning traces from a strong teacher model based on a large pool of questions from NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of factors that affect distilling reasoning capabilities, in terms of sample efficiency and scalability for general reasoning tasks. We observe that simply scaling up data size with random sampling is a strong baseline with steady performance gains. Further, we find that selecting difficult examples that require more diverse reasoning strategies is more sample-efficient to transfer the teacher model's reasoning skills. Evaluated on both Llama and Qwen models, training with NaturalThoughts outperforms existing reasoning datasets such as OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including GPQA-Diamond, MMLU-Pro and SuperGPQA.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yang Li",
      "Youssef Emad",
      "Karthik Padthe",
      "Jack Lanchantin",
      "Weizhe Yuan",
      "Thao Nguyen",
      "Jason Weston",
      "Shang-Wen Li",
      "Dong Wang",
      "Ilia Kulikov",
      "Xian Li"
    ],
    "Date Updated": "2025-07-02",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2507.01921v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window",
    "Reading Status": "Read",
    "Date Published": "2024-07-16",
    "Link": "https://arxiv.org/abs/2407.11963",
    "Topics": [
      "Context Window"
    ],
    "Description": "In evaluating the long-context capabilities of large language models (LLMs), identifying content relevant to a user's query from original long documents is a crucial prerequisite for any LLM to answer questions based on long text. We present NeedleBench, a framework consisting of a series of progressively more challenging tasks for assessing bilingual long-context capabilities, spanning multiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and different depth ranges, allowing the strategic insertion of critical data points in different text depth zones to rigorously test the retrieval and reasoning capabilities of models in diverse contexts. We use the NeedleBench framework to assess how well the leading open-source models can identify key information relevant to the question and apply that information to reasoning in bilingual long texts. Furthermore, we propose the Ancestral Trace Challenge (ATC) to mimic the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks, providing a simple method for evaluating LLMs in dealing with complex long-context situations. Our results suggest that current LLMs have significant room for improvement in practical long-context applications, as they struggle with the complexity of logical reasoning challenges that are likely to be present in real-world long-context tasks. All codes and resources are available at OpenCompass: this https URL. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Mo Li",
      "Songyang Zhang",
      "Taolin Zhang",
      "Haodong Duan",
      "Yunxin Liu",
      "Kai Chen"
    ],
    "Date Updated": "2025-09-17",
    "Comment": "v3: Revisions with added experiments, clarifications, and related\n  work updates",
    "PDF Link": "http://arxiv.org/pdf/2407.11963v3",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "3"
  },
  {
    "Title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-16",
    "Link": "https://arxiv.org/abs/2501.09751",
    "Topics": [],
    "Description": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Zekun Xi",
      "Wenbiao Yin",
      "Jizhan Fang",
      "Jialong Wu",
      "Runnan Fang",
      "Jiang Yong",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "Date Updated": "2025-09-08",
    "Comment": "EMNLP 2025",
    "PDF Link": "http://arxiv.org/pdf/2501.09751v3",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "Version": "3"
  },
  {
    "Title": "OpenAI o1 System Card",
    "Reading Status": "Want to Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2412.16720",
    "Topics": [
      "OpenAI",
      "Technical Report"
    ],
    "Description": "The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "OpenAI",
      ":",
      "Aaron Jaech",
      "Adam Kalai",
      "Adam Lerer",
      "Adam Richardson",
      "Ahmed El-Kishky",
      "Aiden Low",
      "Alec Helyar",
      "Aleksander Madry",
      "Alex Beutel",
      "Alex Carney",
      "Alex Iftimie",
      "Alex Karpenko",
      "Alex Tachard Passos",
      "Alexander Neitz",
      "Alexander Prokofiev",
      "Alexander Wei",
      "Allison Tam",
      "Ally Bennett",
      "Ananya Kumar",
      "Andre Saraiva",
      "Andrea Vallone",
      "Andrew Duberstein",
      "Andrew Kondrich",
      "Andrey Mishchenko",
      "Andy Applebaum",
      "Angela Jiang",
      "Ashvin Nair",
      "Barret Zoph",
      "Behrooz Ghorbani",
      "Ben Rossen",
      "Benjamin Sokolowsky",
      "Boaz Barak",
      "Bob McGrew",
      "Borys Minaiev",
      "Botao Hao",
      "Bowen Baker",
      "Brandon Houghton",
      "Brandon McKinzie",
      "Brydon Eastman",
      "Camillo Lugaresi",
      "Cary Bassin",
      "Cary Hudson",
      "Chak Ming Li",
      "Charles de Bourcy",
      "Chelsea Voss",
      "Chen Shen",
      "Chong Zhang",
      "Chris Koch",
      "Chris Orsinger",
      "Christopher Hesse",
      "Claudia Fischer",
      "Clive Chan",
      "Dan Roberts",
      "Daniel Kappler",
      "Daniel Levy",
      "Daniel Selsam",
      "David Dohan",
      "David Farhi",
      "David Mely",
      "David Robinson",
      "Dimitris Tsipras",
      "Doug Li",
      "Dragos Oprica",
      "Eben Freeman",
      "Eddie Zhang",
      "Edmund Wong",
      "Elizabeth Proehl",
      "Enoch Cheung",
      "Eric Mitchell",
      "Eric Wallace",
      "Erik Ritter",
      "Evan Mays",
      "Fan Wang",
      "Felipe Petroski Such",
      "Filippo Raso",
      "Florencia Leoni",
      "Foivos Tsimpourlas",
      "Francis Song",
      "Fred von Lohmann",
      "Freddie Sulit",
      "Geoff Salmon",
      "Giambattista Parascandolo",
      "Gildas Chabot",
      "Grace Zhao",
      "Greg Brockman",
      "Guillaume Leclerc",
      "Hadi Salman",
      "Haiming Bao",
      "Hao Sheng",
      "Hart Andrin",
      "Hessam Bagherinezhad",
      "Hongyu Ren",
      "Hunter Lightman",
      "Hyung Won Chung",
      "Ian Kivlichan",
      "Ian O'Connell",
      "Ian Osband",
      "Ignasi Clavera Gilaberte",
      "Ilge Akkaya",
      "Ilya Kostrikov",
      "Ilya Sutskever",
      "Irina Kofman",
      "Jakub Pachocki",
      "James Lennon",
      "Jason Wei",
      "Jean Harb",
      "Jerry Twore",
      "Jiacheng Feng",
      "Jiahui Yu",
      "Jiayi Weng",
      "Jie Tang",
      "Jieqi Yu",
      "Joaquin Quiñonero Candela",
      "Joe Palermo",
      "Joel Parish",
      "Johannes Heidecke",
      "John Hallman",
      "John Rizzo",
      "Jonathan Gordon",
      "Jonathan Uesato",
      "Jonathan Ward",
      "Joost Huizinga",
      "Julie Wang",
      "Kai Chen",
      "Kai Xiao",
      "Karan Singhal",
      "Karina Nguyen",
      "Karl Cobbe",
      "Katy Shi",
      "Kayla Wood",
      "Kendra Rimbach",
      "Keren Gu-Lemberg",
      "Kevin Liu",
      "Kevin Lu",
      "Kevin Stone",
      "Kevin Yu",
      "Lama Ahmad",
      "Lauren Yang",
      "Leo Liu",
      "Leon Maksin",
      "Leyton Ho",
      "Liam Fedus",
      "Lilian Weng",
      "Linden Li",
      "Lindsay McCallum",
      "Lindsey Held",
      "Lorenz Kuhn",
      "Lukas Kondraciuk",
      "Lukasz Kaiser",
      "Luke Metz",
      "Madelaine Boyd",
      "Maja Trebacz",
      "Manas Joglekar",
      "Mark Chen",
      "Marko Tintor",
      "Mason Meyer",
      "Matt Jones",
      "Matt Kaufer",
      "Max Schwarzer",
      "Meghan Shah",
      "Mehmet Yatbaz",
      "Melody Y. Guan",
      "Mengyuan Xu",
      "Mengyuan Yan",
      "Mia Glaese",
      "Mianna Chen",
      "Michael Lampe",
      "Michael Malek",
      "Michele Wang",
      "Michelle Fradin",
      "Mike McClay",
      "Mikhail Pavlov",
      "Miles Wang",
      "Mingxuan Wang",
      "Mira Murati",
      "Mo Bavarian",
      "Mostafa Rohaninejad",
      "Nat McAleese",
      "Neil Chowdhury",
      "Neil Chowdhury",
      "Nick Ryder",
      "Nikolas Tezak",
      "Noam Brown",
      "Ofir Nachum",
      "Oleg Boiko",
      "Oleg Murk",
      "Olivia Watkins",
      "Patrick Chao",
      "Paul Ashbourne",
      "Pavel Izmailov",
      "Peter Zhokhov",
      "Rachel Dias",
      "Rahul Arora",
      "Randall Lin",
      "Rapha Gontijo Lopes",
      "Raz Gaon",
      "Reah Miyara",
      "Reimar Leike",
      "Renny Hwang",
      "Rhythm Garg",
      "Robin Brown",
      "Roshan James",
      "Rui Shu",
      "Ryan Cheu",
      "Ryan Greene",
      "Saachi Jain",
      "Sam Altman",
      "Sam Toizer",
      "Sam Toyer",
      "Samuel Miserendino",
      "Sandhini Agarwal",
      "Santiago Hernandez",
      "Sasha Baker",
      "Scott McKinney",
      "Scottie Yan",
      "Shengjia Zhao",
      "Shengli Hu",
      "Shibani Santurkar",
      "Shraman Ray Chaudhuri",
      "Shuyuan Zhang",
      "Siyuan Fu",
      "Spencer Papay",
      "Steph Lin",
      "Suchir Balaji",
      "Suvansh Sanjeev",
      "Szymon Sidor",
      "Tal Broda",
      "Aidan Clark",
      "Tao Wang",
      "Taylor Gordon",
      "Ted Sanders",
      "Tejal Patwardhan",
      "Thibault Sottiaux",
      "Thomas Degry",
      "Thomas Dimson",
      "Tianhao Zheng",
      "Timur Garipov",
      "Tom Stasi",
      "Trapit Bansal",
      "Trevor Creech",
      "Troy Peterson",
      "Tyna Eloundou",
      "Valerie Qi",
      "Vineet Kosaraju",
      "Vinnie Monaco",
      "Vitchyr Pong",
      "Vlad Fomenko",
      "Weiyi Zheng",
      "Wenda Zhou",
      "Wes McCabe",
      "Wojciech Zaremba",
      "Yann Dubois",
      "Yinghai Lu",
      "Yining Chen",
      "Young Cha",
      "Yu Bai",
      "Yuchen He",
      "Yuchen Zhang",
      "Yunyun Wang",
      "Zheng Shao",
      "Zhuohan Li"
    ],
    "Date Updated": "2024-12-21",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.16720v1",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
    "Reading Status": "Read",
    "Date Published": "2024-12-04",
    "Link": "https://www.arxiv.org/abs/2412.03555",
    "Topics": [
      "VLM",
      "Technical Report"
    ],
    "Description": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Andreas Steiner",
      "André Susano Pinto",
      "Michael Tschannen",
      "Daniel Keysers",
      "Xiao Wang",
      "Yonatan Bitton",
      "Alexey Gritsenko",
      "Matthias Minderer",
      "Anthony Sherbondy",
      "Shangbang Long",
      "Siyang Qin",
      "Reeve Ingle",
      "Emanuele Bugliarello",
      "Sahar Kazemzadeh",
      "Thomas Mesnard",
      "Ibrahim Alabdulmohsin",
      "Lucas Beyer",
      "Xiaohua Zhai"
    ],
    "Date Updated": "2024-12-04",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.03555v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV"
    ],
    "Version": "1"
  },
  {
    "Title": "Performance Prediction for Large Systems via Text-to-Text Regression",
    "Reading Status": "Reading",
    "Date Published": "2025-06-26",
    "Link": "https://arxiv.org/abs/2506.21718",
    "Topics": [
      "Metric"
    ],
    "Description": "In many industries, predicting metric outcomes of large systems is a fundamental problem, driven largely by traditional tabular regression. However, such methods struggle on complex systems data in the wild such as configuration files or system logs, where feature engineering is often infeasible. We propose text-to-text regression as a general, scalable alternative. For predicting resource efficiency on Borg, Google's massive compute cluster scheduling system, a 60M parameter encoder-decoder, trained from random initialization, achieves up to a near perfect 0.99 (0.9 average) rank correlation across the entire fleet, and 100x lower MSE than tabular approaches. The model also easily adapts to new tasks in only 500 few-shot examples and captures the densities of complex outcome distributions. Ablation studies highlight the importance of using encoders, increasing sequence length, and the model's inherent uncertainty quantification. These findings pave the way for universal simulators of real-world outcomes.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yash Akhauri",
      "Bryan Lewandowski",
      "Cheng-Hsi Lin",
      "Adrian N. Reyes",
      "Grant C. Forbes",
      "Arissa Wongpanich",
      "Bangding Yang",
      "Mohamed S. Abdelfattah",
      "Sagi Perel",
      "Xingyou Song"
    ],
    "Date Updated": "2025-06-26",
    "Comment": "Code can be found at https://github.com/google-deepmind/regress-lm",
    "PDF Link": "http://arxiv.org/pdf/2506.21718v1",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF",
      "cs.SE",
      "cs.SY",
      "eess.SY"
    ],
    "Version": "1"
  },
  {
    "Title": "Phi-4 Technical Report",
    "Reading Status": "Read",
    "Date Published": "2024-12-12",
    "Link": "https://arxiv.org/abs/2412.08905",
    "Topics": [
      "Technical Report"
    ],
    "Description": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Marah Abdin",
      "Jyoti Aneja",
      "Harkirat Behl",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Suriya Gunasekar",
      "Michael Harrison",
      "Russell J. Hewett",
      "Mojan Javaheripi",
      "Piero Kauffmann",
      "James R. Lee",
      "Yin Tat Lee",
      "Yuanzhi Li",
      "Weishung Liu",
      "Caio C. T. Mendes",
      "Anh Nguyen",
      "Eric Price",
      "Gustavo de Rosa",
      "Olli Saarikivi",
      "Adil Salim",
      "Shital Shah",
      "Xin Wang",
      "Rachel Ward",
      "Yue Wu",
      "Dingli Yu",
      "Cyril Zhang",
      "Yi Zhang"
    ],
    "Date Updated": "2024-12-12",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.08905v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "Pretraining on the Test Set Is All You Need",
    "Reading Status": "Read",
    "Date Published": "2023-09-13",
    "Link": "https://arxiv.org/abs/2309.08632",
    "Topics": [],
    "Description": "Inspired by recent work demonstrating the promise of smaller Transformer-based language models pretrained on carefully curated data, we supercharge such approaches by investing heavily in curating a novel, high quality, non-synthetic data mixture based solely on evaluation benchmarks. Using our novel dataset mixture consisting of less than 100 thousand tokens, we pretrain a 1 million parameter transformer-based LLM \\textbf{phi-CTNL} (pronounced ``fictional\") that achieves perfect results across diverse academic benchmarks, strictly outperforming all known foundation models. \\textbf{phi-CTNL} also beats power-law scaling and exhibits a never-before-seen grokking-like ability to accurately predict downstream evaluation benchmarks' canaries.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Rylan Schaeffer"
    ],
    "Date Updated": "2023-09-13",
    "Comment": "3 pages, satire",
    "PDF Link": "http://arxiv.org/pdf/2309.08632v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "Proximal Policy Optimization Algorithms",
    "Reading Status": "Reading",
    "Date Published": "2017-07-20",
    "Link": "https://arxiv.org/abs/1707.06347",
    "Topics": [
      "RLHF",
      "PPO",
      "OpenAI"
    ],
    "Description": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-16",
    "WeekDay": "Saturday",
    "Week": 33,
    "Authors": [
      "John Schulman",
      "Filip Wolski",
      "Prafulla Dhariwal",
      "Alec Radford",
      "Oleg Klimov"
    ],
    "Date Updated": "2017-08-28",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/1707.06347v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "QLoRA: Efficient Finetuning of Quantized LLMs",
    "Reading Status": "Read",
    "Date Published": "2023-05-23",
    "Link": "https://arxiv.org/abs/2305.14314",
    "Topics": [
      "Fine-tuning",
      "LoRA",
      "Quantization"
    ],
    "Description": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training. ",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tim Dettmers",
      "Artidoro Pagnoni",
      "Ari Holtzman",
      "Luke Zettlemoyer"
    ],
    "Date Updated": "2023-05-23",
    "Comment": "Extended NeurIPS submission",
    "PDF Link": "http://arxiv.org/pdf/2305.14314v1",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Qwen2.5 Technical Report",
    "Reading Status": "Read",
    "Date Published": "2024-12-19",
    "Link": "https://arxiv.org/abs/2412.15115",
    "Topics": [
      "Technical Report"
    ],
    "Description": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Qwen",
      ":",
      "An Yang",
      "Baosong Yang",
      "Beichen Zhang",
      "Binyuan Hui",
      "Bo Zheng",
      "Bowen Yu",
      "Chengyuan Li",
      "Dayiheng Liu",
      "Fei Huang",
      "Haoran Wei",
      "Huan Lin",
      "Jian Yang",
      "Jianhong Tu",
      "Jianwei Zhang",
      "Jianxin Yang",
      "Jiaxi Yang",
      "Jingren Zhou",
      "Junyang Lin",
      "Kai Dang",
      "Keming Lu",
      "Keqin Bao",
      "Kexin Yang",
      "Le Yu",
      "Mei Li",
      "Mingfeng Xue",
      "Pei Zhang",
      "Qin Zhu",
      "Rui Men",
      "Runji Lin",
      "Tianhao Li",
      "Tianyi Tang",
      "Tingyu Xia",
      "Xingzhang Ren",
      "Xuancheng Ren",
      "Yang Fan",
      "Yang Su",
      "Yichang Zhang",
      "Yu Wan",
      "Yuqiong Liu",
      "Zeyu Cui",
      "Zhenru Zhang",
      "Zihan Qiu"
    ],
    "Date Updated": "2025-01-03",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.15115v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "Qwen2.5-VL Technical Report",
    "Reading Status": "Want to Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2502.13923",
    "Topics": [
      "Technical Report"
    ],
    "Description": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Shuai Bai",
      "Keqin Chen",
      "Xuejing Liu",
      "Jialin Wang",
      "Wenbin Ge",
      "Sibo Song",
      "Kai Dang",
      "Peng Wang",
      "Shijie Wang",
      "Jun Tang",
      "Humen Zhong",
      "Yuanzhi Zhu",
      "Mingkun Yang",
      "Zhaohai Li",
      "Jianqiang Wan",
      "Pengfei Wang",
      "Wei Ding",
      "Zheren Fu",
      "Yiheng Xu",
      "Jiabo Ye",
      "Xi Zhang",
      "Tianbao Xie",
      "Zesen Cheng",
      "Hang Zhang",
      "Zhibo Yang",
      "Haiyang Xu",
      "Junyang Lin"
    ],
    "Date Updated": "2025-02-19",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2502.13923v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "Qwen3 Technical Report",
    "Reading Status": "Read",
    "Date Published": "",
    "Link": "https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf",
    "Topics": [
      "Technical Report"
    ],
    "Description": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3\ncomprises a series of large language models (LLMs) designed to advance performance,\nefficiency, and multilingual capabilities. The Qwen3 series includes models of both dense\nand Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to\n235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex,\nmulti-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a\nunified framework. This eliminates the need to switch between different models—–such\nas chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ\u000232B)—–and enables dynamic mode switching based on user queries or chat templates.\nMeanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate\ncomputational resources adaptively during inference, thereby balancing latency and\nperformance based on task complexity. Moreover, by leveraging the knowledge from the\nflagship models, we significantly reduce the computational resources required to build\nsmaller-scale models, while ensuring their highly competitive performance. Empirical\nevaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse\nbenchmarks, including tasks in code generation, mathematical reasoning, agent tasks,\netc., competitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages\nand dialects, enhancing global accessibility through improved cross-lingual understand\u0002ing and generation capabilities. To facilitate reproducibility and community-driven\nresearch and development, all Qwen3 models are publicly accessible under Apache 2.0.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [],
    "Date Updated": null,
    "Comment": NaN,
    "PDF Link": NaN,
    "Primary Category": NaN,
    "All Categories": [],
    "Version": NaN
  },
  {
    "Title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with\n  Reinforcement Learning",
    "Reading Status": "Read",
    "Date Published": "2025-05-23",
    "Link": "https://arxiv.org/abs/2505.17667",
    "Topics": [
      "RLHF",
      "Context Window"
    ],
    "Description": "Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Fanqi Wan",
      "Weizhou Shen",
      "Shengyi Liao",
      "Yingcheng Shi",
      "Chenliang Li",
      "Ziyi Yang",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou",
      "Ming Yan"
    ],
    "Date Updated": "2025-05-27",
    "Comment": "Technical Report",
    "PDF Link": "http://arxiv.org/pdf/2505.17667v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
    "Reading Status": "Read",
    "Date Published": "2025-05-26",
    "Link": "https://arxiv.org/abs/2505.20046",
    "Topics": [
      "Reranker"
    ],
    "Description": "We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Le Zhang",
      "Bo Wang",
      "Xipeng Qiu",
      "Siva Reddy",
      "Aishwarya Agrawal"
    ],
    "Date Updated": "2025-05-26",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.20046v1",
    "Primary Category": "cs.IR",
    "All Categories": [
      "cs.IR",
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "Reasoning Models Don't Always Say What They Think",
    "Reading Status": "Reading",
    "Date Published": "2025-05-08",
    "Link": "https://arxiv.org/abs/2505.05410",
    "Topics": [
      "COT"
    ],
    "Description": "Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models' actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT monitoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yanda Chen",
      "Joe Benton",
      "Ansh Radhakrishnan",
      "Jonathan Uesato",
      "Carson Denison",
      "John Schulman",
      "Arushi Somani",
      "Peter Hase",
      "Misha Wagner",
      "Fabien Roger",
      "Vlad Mikulik",
      "Samuel R. Bowman",
      "Jan Leike",
      "Jared Kaplan",
      "Ethan Perez"
    ],
    "Date Updated": "2025-05-08",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.05410v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "Recursive Language Models",
    "Reading Status": "Reading",
    "Date Published": "2025-12-31",
    "Link": "https://arxiv.org/abs/2512.24601",
    "Topics": [
      "RLM"
    ],
    "Description": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
    "Month": NaN,
    "Date Read": "",
    "Date Added": "2026-01-15",
    "WeekDay": NaN,
    "Week": null,
    "Authors": [],
    "Date Updated": "",
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "Reading Status": "Read",
    "Date Published": "2021-04-20",
    "Link": "https://arxiv.org/abs/2104.09864",
    "Topics": [
      "RoPE",
      "Attention Mechanism"
    ],
    "Description": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jianlin Su",
      "Yu Lu",
      "Shengfeng Pan",
      "Ahmed Murtadha",
      "Bo Wen",
      "Yunfeng Liu"
    ],
    "Date Updated": "2023-11-08",
    "Comment": "fixed some typos",
    "PDF Link": "http://arxiv.org/pdf/2104.09864v5",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "5"
  },
  {
    "Title": "Robust Speech Recognition via Large-Scale Weak Supervision",
    "Reading Status": "Read",
    "Date Published": "2022-12-06",
    "Link": "https://arxiv.org/abs/2212.04356",
    "Topics": [
      "Audio",
      "Whisper"
    ],
    "Description": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Alec Radford",
      "Jong Wook Kim",
      "Tao Xu",
      "Greg Brockman",
      "Christine McLeavey",
      "Ilya Sutskever"
    ],
    "Date Updated": "2022-12-06",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2212.04356v1",
    "Primary Category": "eess.AS",
    "All Categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "Version": "1"
  },
  {
    "Title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
    "Reading Status": "Read",
    "Date Published": "2025-03-12",
    "Link": "https://arxiv.org/abs/2503.09516",
    "Topics": [
      "RLHF",
      "Prompt Engineering"
    ],
    "Description": "Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Retrieval augmentation and tool-use training approaches where a search engine is treated as a tool lack complex multi-turn retrieval flexibility or require large-scale supervised data. Prompting advanced LLMs with reasoning capabilities during inference to use search engines is not optimal, since the LLM does not learn how to optimally interact with the search engine. This paper introduces Search-R1, an extension of the DeepSeek-R1 model where the LLM learns -- solely through reinforcement learning (RL) -- to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM rollouts with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Bowen Jin",
      "Hansi Zeng",
      "Zhenrui Yue",
      "Jinsung Yoon",
      "Sercan Arik",
      "Dong Wang",
      "Hamed Zamani",
      "Jiawei Han"
    ],
    "Date Updated": "2025-08-05",
    "Comment": "31 pages",
    "PDF Link": "http://arxiv.org/pdf/2503.09516v5",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "Version": "5"
  },
  {
    "Title": "Sigmoid Loss for Language Image Pre-Training",
    "Reading Status": "Read",
    "Date Published": "2023-03-27",
    "Link": "https://arxiv.org/abs/2303.15343",
    "Topics": [
      "Computer Vision",
      "CLIP"
    ],
    "Description": "We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at this https URL and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Xiaohua Zhai",
      "Basil Mustafa",
      "Alexander Kolesnikov",
      "Lucas Beyer"
    ],
    "Date Updated": "2023-09-27",
    "Comment": "ICCV'23 Oral. arXiv v2: fix typo in pseudocode; v3: clarify t vs t'\n  init; v4: add SigLIP Base, Large, Shape-Optimized 400M results. Models\n  released at: https://github.com/google-research/big_vision. Xiaohua and Lucas\n  contributed equally",
    "PDF Link": "http://arxiv.org/pdf/2303.15343v4",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.AI"
    ],
    "Version": "4"
  },
  {
    "Title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "Reading Status": "Want to Read",
    "Date Published": "2024-12-18",
    "Link": "https://arxiv.org/abs/2412.13663",
    "Topics": [],
    "Description": "Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Benjamin Warner",
      "Antoine Chaffin",
      "Benjamin Clavié",
      "Orion Weller",
      "Oskar Hallström",
      "Said Taghadouini",
      "Alexis Gallagher",
      "Raja Biswas",
      "Faisal Ladhak",
      "Tom Aarsen",
      "Nathan Cooper",
      "Griffin Adams",
      "Jeremy Howard",
      "Iacopo Poli"
    ],
    "Date Updated": "2024-12-19",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.13663v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI"
    ],
    "Version": "2"
  },
  {
    "Title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train\n  One Large Neural Network",
    "Reading Status": "Read",
    "Date Published": "2021-04-05",
    "Link": "https://arxiv.org/abs/2104.02133",
    "Topics": [
      "Audio"
    ],
    "Description": "We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "William Chan",
      "Daniel Park",
      "Chris Lee",
      "Yu Zhang",
      "Quoc Le",
      "Mohammad Norouzi"
    ],
    "Date Updated": "2021-04-27",
    "Comment": "submitted to INTERSPEECH",
    "PDF Link": "http://arxiv.org/pdf/2104.02133v3",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.LG"
    ],
    "Version": "3"
  },
  {
    "Title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction",
    "Reading Status": "Read",
    "Date Published": "2025-02-17",
    "Link": "https://arxiv.org/html/2502.11946v1",
    "Topics": [
      "TTS",
      "ASR",
      "Speech",
      "Technical Report",
      "Audio"
    ],
    "Description": "Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at https://github.com/stepfun-ai/Step-Audio.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Ailin Huang",
      "Boyong Wu",
      "Bruce Wang",
      "Chao Yan",
      "Chen Hu",
      "Chengli Feng",
      "Fei Tian",
      "Feiyu Shen",
      "Jingbei Li",
      "Mingrui Chen",
      "Peng Liu",
      "Ruihang Miao",
      "Wang You",
      "Xi Chen",
      "Xuerui Yang",
      "Yechang Huang",
      "Yuxiang Zhang",
      "Zheng Gong",
      "Zixin Zhang",
      "Hongyu Zhou",
      "Jianjian Sun",
      "Brian Li",
      "Chengting Feng",
      "Changyi Wan",
      "Hanpeng Hu",
      "Jianchang Wu",
      "Jiangjie Zhen",
      "Ranchen Ming",
      "Song Yuan",
      "Xuelin Zhang",
      "Yu Zhou",
      "Bingxin Li",
      "Buyun Ma",
      "Hongyuan Wang",
      "Kang An",
      "Wei Ji",
      "Wen Li",
      "Xuan Wen",
      "Xiangwen Kong",
      "Yuankai Ma",
      "Yuanwei Liang",
      "Yun Mou",
      "Bahtiyar Ahmidi",
      "Bin Wang",
      "Bo Li",
      "Changxin Miao",
      "Chen Xu",
      "Chenrun Wang",
      "Dapeng Shi",
      "Deshan Sun",
      "Dingyuan Hu",
      "Dula Sai",
      "Enle Liu",
      "Guanzhe Huang",
      "Gulin Yan",
      "Heng Wang",
      "Haonan Jia",
      "Haoyang Zhang",
      "Jiahao Gong",
      "Junjing Guo",
      "Jiashuai Liu",
      "Jiahong Liu",
      "Jie Feng",
      "Jie Wu",
      "Jiaoren Wu",
      "Jie Yang",
      "Jinguo Wang",
      "Jingyang Zhang",
      "Junzhe Lin",
      "Kaixiang Li",
      "Lei Xia",
      "Li Zhou",
      "Liang Zhao",
      "Longlong Gu",
      "Mei Chen",
      "Menglin Wu",
      "Ming Li",
      "Mingxiao Li",
      "Mingliang Li",
      "Mingyao Liang",
      "Na Wang",
      "Nie Hao",
      "Qiling Wu",
      "Qinyuan Tan",
      "Ran Sun",
      "Shuai Shuai",
      "Shaoliang Pang",
      "Shiliang Yang",
      "Shuli Gao",
      "Shanshan Yuan",
      "Siqi Liu",
      "Shihong Deng",
      "Shilei Jiang",
      "Sitong Liu",
      "Tiancheng Cao",
      "Tianyu Wang",
      "Wenjin Deng",
      "Wuxun Xie",
      "Weipeng Ming",
      "Wenqing He",
      "Wen Sun",
      "Xin Han",
      "Xin Huang",
      "Xiaomin Deng",
      "Xiaojia Liu",
      "Xin Wu",
      "Xu Zhao",
      "Yanan Wei",
      "Yanbo Yu",
      "Yang Cao",
      "Yangguang Li",
      "Yangzhen Ma",
      "Yanming Xu",
      "Yaoyu Wang",
      "Yaqiang Shi",
      "Yilei Wang",
      "Yizhuang Zhou",
      "Yinmin Zhong",
      "Yang Zhang",
      "Yaoben Wei",
      "Yu Luo",
      "Yuanwei Lu",
      "Yuhe Yin",
      "Yuchu Luo",
      "Yuanhao Ding",
      "Yuting Yan",
      "Yaqi Dai",
      "Yuxiang Yang",
      "Zhe Xie",
      "Zheng Ge",
      "Zheng Sun",
      "Zhewei Huang",
      "Zhichao Chang",
      "Zhisheng Guan",
      "Zidong Yang",
      "Zili Zhang",
      "Binxing Jiao",
      "Daxin Jiang",
      "Heung-Yeung Shum",
      "Jiansheng Chen",
      "Jing Li",
      "Shuchang Zhou",
      "Xiangyu Zhang",
      "Xinhao Zhang",
      "Yibo Zhu"
    ],
    "Date Updated": "2025-02-18",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2502.11946v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "Version": "2"
  },
  {
    "Title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback",
    "Reading Status": "Read",
    "Date Published": "2025-01-22",
    "Link": "https://arxiv.org/abs/2501.12895",
    "Topics": [
      "RLHF",
      "DPO",
      "TPO"
    ],
    "Description": "Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at https://github.com/yafuly/TPO.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yafu Li",
      "Xuyang Hu",
      "Xiaoye Qu",
      "Linjie Li",
      "Yu Cheng"
    ],
    "Date Updated": "2025-01-22",
    "Comment": "43 pages; work in progress",
    "PDF Link": "http://arxiv.org/pdf/2501.12895v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
    "Reading Status": "Read",
    "Date Published": "2024-02-27",
    "Link": "https://arxiv.org/abs/2402.17764",
    "Topics": [
      "Quantization"
    ],
    "Description": "Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Shuming Ma",
      "Hongyu Wang",
      "Lingxiao Ma",
      "Lei Wang",
      "Wenhui Wang",
      "Shaohan Huang",
      "Li Dong",
      "Ruiping Wang",
      "Jilong Xue",
      "Furu Wei"
    ],
    "Date Updated": "2024-02-27",
    "Comment": "Work in progress",
    "PDF Link": "http://arxiv.org/pdf/2402.17764v1",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.LG"
    ],
    "Version": "1"
  },
  {
    "Title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "Reading Status": "Read",
    "Date Published": "2025-06-07",
    "Link": "https://arxiv.org/abs/2506.06941",
    "Topics": [
      "COT"
    ],
    "Description": "Recent generations of language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established math and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from contamination and does not provide insights into the reasoning traces. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs think. Through extensive experiments, we show that LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having remaining token budget. By comparing LRMs with their standard LLM counterparts under same inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models outperform LRMs, (2) medium-complexity tasks where LRMs demonstrates advantage, and (3) high-complexity tasks where both models face complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and raising questions about their reasoning capabilities.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Parshin Shojaee",
      "Iman Mirzadeh",
      "Keivan Alizadeh",
      "Maxwell Horton",
      "Samy Bengio",
      "Mehrdad Farajtabar"
    ],
    "Date Updated": "2025-07-18",
    "Comment": "preprint",
    "PDF Link": "http://arxiv.org/pdf/2506.06941v2",
    "Primary Category": "cs.AI",
    "All Categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "The Perfect Blend: Redefining RLHF with Mixture of Judges",
    "Reading Status": "Read",
    "Date Published": "2024-09-30",
    "Link": "https://arxiv.org/abs/2409.20370",
    "Topics": [
      "RLHF",
      "Mixture of Judges"
    ],
    "Description": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.\nOur empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Tengyu Xu",
      "Eryk Helenowski",
      "Karthik Abinav Sankararaman",
      "Di Jin",
      "Kaiyan Peng",
      "Eric Han",
      "Shaoliang Nie",
      "Chen Zhu",
      "Hejia Zhang",
      "Wenxuan Zhou",
      "Zhouhao Zeng",
      "Yun He",
      "Karishma Mandyam",
      "Arya Talabzadeh",
      "Madian Khabsa",
      "Gabriel Cohen",
      "Yuandong Tian",
      "Hao Ma",
      "Sinong Wang",
      "Han Fang"
    ],
    "Date Updated": "2024-09-30",
    "Comment": "submitted to conference",
    "PDF Link": "http://arxiv.org/pdf/2409.20370v1",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "Version": "1"
  },
  {
    "Title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "Reading Status": "Read",
    "Date Published": "2024-12-09",
    "Link": "https://arxiv.org/abs/2412.06769",
    "Topics": [
      "COT",
      "COCONUT"
    ],
    "Description": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Shibo Hao",
      "Sainbayar Sukhbaatar",
      "DiJia Su",
      "Xian Li",
      "Zhiting Hu",
      "Jason Weston",
      "Yuandong Tian"
    ],
    "Date Updated": "2024-12-11",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2412.06769v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "Transformers without Normalization",
    "Reading Status": "Read",
    "Date Published": "2025-03-13",
    "Link": "https://arxiv.org/abs/2503.10622",
    "Topics": [
      "Normalization"
    ],
    "Description": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Jiachen Zhu",
      "Xinlei Chen",
      "Kaiming He",
      "Yann LeCun",
      "Zhuang Liu"
    ],
    "Date Updated": "2025-06-14",
    "Comment": "CVPR 2025; Project page: https://jiachenzhu.github.io/DyT/",
    "PDF Link": "http://arxiv.org/pdf/2503.10622v2",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "Version": "2"
  },
  {
    "Title": "Trust Region Policy Optimization",
    "Reading Status": "Read",
    "Date Published": "2017-04-20",
    "Link": "https://arxiv.org/abs/1502.05477",
    "Topics": [
      "RLHF",
      "PPO",
      "TRPO"
    ],
    "Description": "We describe an iterative procedure for optimizing\npolicies, with guaranteed monotonic improve\u0002ment. By making several approximations to the\ntheoretically-justified procedure, we develop a\npractical algorithm, called Trust Region Policy\nOptimization (TRPO). This algorithm is similar\nto natural policy gradient methods and is effec\u0002tive for optimizing large nonlinear policies such\nas neural networks. Our experiments demon\u0002strate its robust performance on a wide variety\nof tasks: learning simulated robotic swimming,\nhopping, and walking gaits; and playing Atari\ngames using images of the screen as input. De\u0002spite its approximations that deviate from the\ntheory, TRPO tends to give monotonic improve\u0002ment, with little tuning of hyperparameters",
    "Month": "2025-08",
    "Date Read": "2025-08-24",
    "Date Added": "2025-08-18",
    "WeekDay": "Monday",
    "Week": 34,
    "Authors": [
      "John Schulman",
      "Sergey Levine",
      "Philipp Moritz",
      "Michael I. Jordan",
      "Pieter Abbeel"
    ],
    "Date Updated": "2017-04-20",
    "Comment": "16 pages, ICML 2015",
    "PDF Link": "http://arxiv.org/pdf/1502.05477v5",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG"
    ],
    "Version": "5"
  },
  {
    "Title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus",
    "Reading Status": "Want to Read",
    "Date Published": "2025-01-10",
    "Link": "https://arxiv.org/abs/2501.05874",
    "Topics": [],
    "Description": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Soyeong Jeong",
      "Kangsan Kim",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "Date Updated": "2025-05-28",
    "Comment": "ACL Findings 2025",
    "PDF Link": "http://arxiv.org/pdf/2501.05874v3",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "Version": "3"
  },
  {
    "Title": "Vision Mamba: Efficient Visual Representation Learning with\n  Bidirectional State Space Model",
    "Reading Status": "Reading",
    "Date Published": "2024-01-17",
    "Link": "https://arxiv.org/abs/2401.09417",
    "Topics": [
      "Mamba",
      "S6",
      "Computer Vision"
    ],
    "Description": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation & memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation & memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.",
    "Month": "2025-10",
    "Date Read": "",
    "Date Added": "2025-10-12",
    "WeekDay": "Sunday",
    "Week": 41,
    "Authors": [],
    "Date Updated": null,
    "Comment": "",
    "PDF Link": "",
    "Primary Category": "",
    "All Categories": [],
    "Version": ""
  },
  {
    "Title": "Were RNNs All We Needed?",
    "Reading Status": "Read",
    "Date Published": "2024-10-02",
    "Link": "https://arxiv.org/abs/2410.01201",
    "Topics": [
      "RNN",
      "LSTM",
      "GRU"
    ],
    "Description": "The introduction of Transformers in 2017 reshaped the landscape of deep learning. Originally proposed for sequence modelling, Transformers have since achieved widespread success across various domains. However, the scalability limitations of Transformers - particularly with respect to sequence length - have sparked renewed interest in novel recurrent models that are parallelizable during training, offer comparable performance, and scale more effectively. In this work, we revisit sequence modelling from a historical perspective, focusing on Recurrent Neural Networks (RNNs), which dominated the field for two decades before the rise of Transformers. Specifically, we examine LSTMs (1997) and GRUs (2014). We demonstrate that by simplifying these models, we can derive minimal versions (minLSTMs and minGRUs) that (1) use fewer parameters than their traditional counterparts, (2) are fully parallelizable during training, and (3) achieve surprisingly competitive performance on a range of tasks, rivalling recent models including Transformers.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Leo Feng",
      "Frederick Tung",
      "Mohamed Osama Ahmed",
      "Yoshua Bengio",
      "Hossein Hajimirsadeghi"
    ],
    "Date Updated": "2024-11-28",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2410.01201v3",
    "Primary Category": "cs.LG",
    "All Categories": [
      "cs.LG",
      "cs.AI"
    ],
    "Version": "3"
  },
  {
    "Title": "WorldPM: Scaling Human Preference Modeling",
    "Reading Status": "Want to Read",
    "Date Published": "2025-05-15",
    "Link": "https://arxiv.org/abs/2505.10527",
    "Topics": [
      "RLHF"
    ],
    "Description": "Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Binghai Wang",
      "Runji Lin",
      "Keming Lu",
      "Le Yu",
      "Zhenru Zhang",
      "Fei Huang",
      "Chujie Zheng",
      "Kai Dang",
      "Yang Fan",
      "Xingzhang Ren",
      "An Yang",
      "Binyuan Hui",
      "Dayiheng Liu",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Bowen Yu",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "Date Updated": "2025-05-18",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2505.10527v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL"
    ],
    "Version": "2"
  },
  {
    "Title": "YOLOv10: Real-Time End-to-End Object Detection",
    "Reading Status": "Want to Read",
    "Date Published": "2024-05-23",
    "Link": "https://arxiv.org/abs/2405.14458",
    "Topics": [
      "YOLO",
      "Technical Report"
    ],
    "Description": "Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8× faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8× smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46\\% less latency and 25\\% fewer parameters for the same performance.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Ao Wang",
      "Hui Chen",
      "Lihao Liu",
      "Kai Chen",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "Date Updated": "2024-10-30",
    "Comment": "Code: https://github.com/THU-MIG/yolov10; NeurIPS 2024 Camera-ready\n  Version",
    "PDF Link": "http://arxiv.org/pdf/2405.14458v2",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV"
    ],
    "Version": "2"
  },
  {
    "Title": "YOLOv12: Attention-Centric Real-Time Object Detectors",
    "Reading Status": "Read",
    "Date Published": "2025-02-18",
    "Link": "https://www.arxiv.org/abs/2502.12524",
    "Topics": [
      "YOLO",
      "Technical Report"
    ],
    "Description": "Enhancing the network architecture of the YOLO framework has been crucial for a long time, but has focused on CNN-based improvements despite the proven superiority of attention mechanisms in modeling capabilities. This is because attention-based models cannot match the speed of CNN-based models. This paper proposes an attention-centric YOLO framework, namely YOLOv12, that matches the speed of previous CNN-based ones while harnessing the performance benefits of attention mechanisms. YOLOv12 surpasses all popular real-time object detectors in accuracy with competitive speed. For example, YOLOv12-N achieves 40.6% mAP with an inference latency of 1.64 ms on a T4 GPU, outperforming advanced YOLOv10-N / YOLOv11-N by 2.1%/1.2% mAP with a comparable speed. This advantage extends to other model scales. YOLOv12 also surpasses end-to-end real-time detectors that improve DETR, such as RT-DETR / RT-DETRv2: YOLOv12-S beats RT-DETR-R18 / RT-DETRv2-R18 while running 42% faster, using only 36% of the computation and 45% of the parameters. More comparisons are shown in Figure 1.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Yunjie Tian",
      "Qixiang Ye",
      "David Doermann"
    ],
    "Date Updated": "2025-02-18",
    "Comment": "https://github.com/sunsmarterjie/yolov12",
    "PDF Link": "http://arxiv.org/pdf/2502.12524v1",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV",
      "cs.AI"
    ],
    "Version": "1"
  },
  {
    "Title": "YaRN: Efficient Context Window Extension of Large Language Models",
    "Reading Status": "Read",
    "Date Published": "2023-08-31",
    "Link": "https://arxiv.org/abs/2309.00071",
    "Topics": [
      "Attention Mechanism",
      "Context Window",
      "RoPE",
      "YaRN"
    ],
    "Description": "Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Bowen Peng",
      "Jeffrey Quesnelle",
      "Honglu Fan",
      "Enrico Shippole"
    ],
    "Date Updated": "2023-11-01",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2309.00071v2",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "Version": "2"
  },
  {
    "Title": "You Only Look Once: Unified, Real-Time Object Detection",
    "Reading Status": "Want to Read",
    "Date Published": "2015-06-08",
    "Link": "https://arxiv.org/abs/1506.02640",
    "Topics": [
      "Computer Vision",
      "YOLO"
    ],
    "Description": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.   Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.",
    "Month": "2025-08",
    "Date Read": "",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Joseph Redmon",
      "Santosh Divvala",
      "Ross Girshick",
      "Ali Farhadi"
    ],
    "Date Updated": "2016-05-09",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/1506.02640v5",
    "Primary Category": "cs.CV",
    "All Categories": [
      "cs.CV"
    ],
    "Version": "5"
  },
  {
    "Title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations",
    "Reading Status": "Read",
    "Date Published": "2025-08-15",
    "Link": "https://arxiv.org/abs/2006.11477",
    "Topics": [
      "Audio"
    ],
    "Description": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.",
    "Month": "2025-08",
    "Date Read": "2025-08-15",
    "Date Added": "2025-08-15",
    "WeekDay": "Friday",
    "Week": 33,
    "Authors": [
      "Alexei Baevski",
      "Henry Zhou",
      "Abdelrahman Mohamed",
      "Michael Auli"
    ],
    "Date Updated": "2020-10-22",
    "Comment": NaN,
    "PDF Link": "http://arxiv.org/pdf/2006.11477v3",
    "Primary Category": "cs.CL",
    "All Categories": [
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "Version": "3"
  }
]