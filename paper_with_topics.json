[{"Title": "52B to 1T: Lessons Learned via Tele-FLM Series", "Reading Status": "Read", "Date Added": "2024-07-03", "Link": "https://arxiv.org/abs/2407.02783", "Topics": ["LLM"], "Description": ""}, {"Title": "Aya 23: Open Weight Release to Further Multilingual Progress", "Reading Status": "Read", "Date Added": "2024-05-31", "Link": "https://arxiv.org/abs/2405.15032", "Topics": ["Translation", "Multilingual", "LLM", "Technical Report"], "Description": ""}, {"Title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "Reading Status": "Read", "Date Added": "2019-05-24", "Link": "https://arxiv.org/abs/1810.04805", "Topics": ["LLM", "BERT", "Encoders"], "Description": ""}, {"Title": "Decoding Speculative Decoding", "Reading Status": "Read", "Date Added": "2024-04-26", "Link": "https://arxiv.org/abs/2402.01528", "Topics": ["LLM", "Speculative Decoding"], "Description": ""}, {"Title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "Reading Status": "Read", "Date Added": "2020-03-01", "Link": "https://arxiv.org/abs/1910.01108", "Topics": ["LLM", "Knowledge Distillation", "BERT"], "Description": ""}, {"Title": "Distilling the Knowledge in a Neural Network", "Reading Status": "Read", "Date Added": "2015-03-09", "Link": "https://arxiv.org/abs/1503.02531", "Topics": ["Neural Network", "LLM", "Knowledge Distillation"], "Description": ""}, {"Title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking", "Reading Status": "Reading", "Date Added": "2024-05-28", "Link": "https://arxiv.org/abs/2405.18414", "Topics": ["GraphDB", "LLM", "Reranker", "RAG"], "Description": ""}, {"Title": "Efficient Guided Generation for Large Language Models", "Reading Status": "Want to Read", "Date Added": "2023-08-19", "Link": "https://arxiv.org/abs/2307.09702", "Topics": [], "Description": ""}, {"Title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, Age", "Reading Status": "Read", "Date Added": "2019-04-14", "Link": "https://arxiv.org/abs/1908.04913", "Topics": ["Dataset", "Computer Vision"], "Description": ""}, {"Title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "Reading Status": "Read", "Date Added": "2024-08-20", "Link": "https://arxiv.org/abs/2408.10774", "Topics": ["LoRA", "LLM", "Fine-tuning"], "Description": ""}, {"Title": "GEAR: Graph-enhanced Agent for Retrieval-augmented Generation", "Reading Status": "Want to Read", "Date Added": "2024-12-24", "Link": "https://arxiv.org/abs/2412.18431", "Topics": ["LLM", "GraphDB", "RAG"], "Description": ""}, {"Title": "Image-to-Image Translation with Condotional Adversarial Networks", "Reading Status": "Read", "Date Added": "2018-11-26", "Link": "https://arxiv.org/abs/1611.07004", "Topics": ["Computer Vision", "GAN"], "Description": ""}, {"Title": "Learning Transferable Visual Models From Natural Language Supervision", "Reading Status": "Read", "Date Added": "2021-02-26", "Link": "https://arxiv.org/abs/2103.00020", "Topics": ["Computer Vision", "CLIP"], "Description": ""}, {"Title": "LoRA: Low-Rank Adaptation of Large Language Models", "Reading Status": "Read", "Date Added": "2021-10-16", "Link": "https://arxiv.org/abs/2106.09685", "Topics": ["Fine-tuning", "LLM", "LoRA"], "Description": ""}, {"Title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "Reading Status": "Read", "Date Added": "2024-07-30", "Link": "https://arxiv.org/abs/2407.19594", "Topics": ["LLM", "Judge"], "Description": ""}, {"Title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window", "Reading Status": "Read", "Date Added": "2024-07-16", "Link": "https://arxiv.org/abs/2407.11963", "Topics": ["LLM", "Context Window"], "Description": ""}, {"Title": "OpenAI o1 System Card", "Reading Status": "Want to Read", "Date Added": "2024-09-12", "Link": "", "Topics": ["LLM", "OpenAI", "Technical Report"], "Description": ""}, {"Title": "Phi-4 Technical Report", "Reading Status": "Want to Read", "Date Added": "2024-12-12", "Link": "https://arxiv.org/abs/2412.08905", "Topics": ["LLM", "Technical Report"], "Description": ""}, {"Title": "QLoRA: Efficient Finetuning of Quantized LLMs", "Reading Status": "Read", "Date Added": "2023-05-23", "Link": "https://arxiv.org/abs/2305.14314", "Topics": ["Fine-tuning", "LLM", "LoRA"], "Description": ""}, {"Title": "Sigmoid Loss for Language Image Pre-Training", "Reading Status": "Read", "Date Added": "2023-09-27", "Link": "https://arxiv.org/abs/2303.15343", "Topics": ["Computer Vision", "CLIP"], "Description": ""}, {"Title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference", "Reading Status": "Want to Read", "Date Added": "2024-12-19", "Link": "https://arxiv.org/abs/2412.13663", "Topics": [], "Description": ""}, {"Title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "Reading Status": "Read", "Date Added": "2024-09-30", "Link": "https://arxiv.org/abs/2409.20370", "Topics": ["LLM", "RLHF", "MOJ"], "Description": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.\nOur empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications."}, {"Title": "YOLOv10: Real-Time End-to-End Object Detection", "Reading Status": "Want to Read", "Date Added": "2024-05-23", "Link": "https://arxiv.org/abs/2405.14458", "Topics": [], "Description": ""}, {"Title": "You Only Look Once: Unified, Real-Time Object Detection", "Reading Status": "Want to Read", "Date Added": "2016-05-09", "Link": "https://arxiv.org/abs/1506.02640", "Topics": ["Computer Vision", "YOLO"], "Description": ""}]