[{"Title": "$\\text{Transformer}^2$: Self-adaptive LLMs", "Reading Status": "Want to Read", "Date Added": "2025-01-09", "Link": "https://arxiv.org/abs/2501.06252", "Topics": [], "Description": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce $\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, $\\text{Transformer}^2$ employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. $\\text{Transformer}^2$ represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems."}, {"Title": "52B to 1T: Lessons Learned via Tele-FLM Series", "Reading Status": "Read", "Date Added": "2024-07-03", "Link": "https://arxiv.org/abs/2407.02783", "Topics": ["LLM"], "Description": ""}, {"Title": "Aya 23: Open Weight Release to Further Multilingual Progress", "Reading Status": "Read", "Date Added": "2024-05-31", "Link": "https://arxiv.org/abs/2405.15032", "Topics": ["Translation", "Multilingual", "LLM", "Technical Report"], "Description": ""}, {"Title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "Reading Status": "Read", "Date Added": "2019-05-24", "Link": "https://arxiv.org/abs/1810.04805", "Topics": ["LLM", "BERT", "Encoders"], "Description": ""}, {"Title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning", "Reading Status": "Want to Read", "Date Added": "2025-01-11", "Link": "https://arxiv.org/abs/2501.06590", "Topics": [], "Description": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent"}, {"Title": "Decoding Speculative Decoding", "Reading Status": "Read", "Date Added": "2024-04-26", "Link": "https://arxiv.org/abs/2402.01528", "Topics": ["LLM", "Speculative Decoding"], "Description": ""}, {"Title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "Reading Status": "Reading", "Date Added": "2023-05-29", "Link": "https://arxiv.org/abs/2305.18290", "Topics": ["DPO", "RLHF"], "Description": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."}, {"Title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "Reading Status": "Read", "Date Added": "2020-03-01", "Link": "https://arxiv.org/abs/1910.01108", "Topics": ["LLM", "Knowledge Distillation", "BERT"], "Description": ""}, {"Title": "Distilling the Knowledge in a Neural Network", "Reading Status": "Read", "Date Added": "2015-03-09", "Link": "https://arxiv.org/abs/1503.02531", "Topics": ["Neural Network", "LLM", "Knowledge Distillation"], "Description": ""}, {"Title": "Don't Forget to Connect! Improving RAG with Graph-based Reranking", "Reading Status": "Read", "Date Added": "2024-05-28", "Link": "https://arxiv.org/abs/2405.18414", "Topics": ["GraphDB", "LLM", "Reranker", "RAG"], "Description": "Retrieval Augmented Generation (RAG) has greatly improved the performance of Large Language Model (LLM) responses by grounding generation with context from existing documents. These systems work well when documents are clearly relevant to a question context. But what about when a document has partial information, or less obvious connections to the context? And how should we reason about connections between documents? In this work, we seek to answer these two core questions about RAG generation. We introduce G-RAG, a reranker based on graph neural networks (GNNs) between the retriever and reader in RAG. Our method combines both connections between documents and semantic information (via Abstract Meaning Representation graphs) to provide a context-informed ranker for RAG. G-RAG outperforms state-of-the-art approaches while having smaller computational footprint. Additionally, we assess the performance of PaLM 2 as a reranker and find it to significantly underperform G-RAG. This result emphasizes the importance of reranking for RAG even when using Large Language Models. "}, {"Title": "Efficient Guided Generation for Large Language Models", "Reading Status": "Want to Read", "Date Added": "2023-08-19", "Link": "https://arxiv.org/abs/2307.09702", "Topics": [], "Description": ""}, {"Title": "Enhancing Retrieval-Augmented Generation: A Study of Best Practices", "Reading Status": "Want to Read", "Date Added": "2025-01-13", "Link": "https://arxiv.org/abs/2501.07391", "Topics": [], "Description": "Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios. Our code and implementation details are publicly available."}, {"Title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, Age", "Reading Status": "Read", "Date Added": "2019-04-14", "Link": "https://arxiv.org/abs/1908.04913", "Topics": ["Dataset", "Computer Vision"], "Description": ""}, {"Title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "Reading Status": "Read", "Date Added": "2024-08-20", "Link": "https://arxiv.org/abs/2408.10774", "Topics": ["LoRA", "LLM", "Fine-tuning"], "Description": ""}, {"Title": "Foundations of Large Language Models", "Reading Status": "Want to Read", "Date Added": "2025-01-16", "Link": "https://arxiv.org/abs/2501.09223", "Topics": [], "Description": "This is a book about large language models. As indicated by the title, it primarily focuses on foundational concepts rather than comprehensive coverage of all cutting-edge technologies. The book is structured into four main chapters, each exploring a key area: pre-training, generative models, prompting techniques, and alignment methods. It is intended for college students, professionals, and practitioners in natural language processing and related fields, and can serve as a reference for anyone interested in large language models."}, {"Title": "GEAR: Graph-enhanced Agent for Retrieval-augmented Generation", "Reading Status": "Want to Read", "Date Added": "2024-12-24", "Link": "https://arxiv.org/abs/2412.18431", "Topics": ["LLM", "GraphDB", "RAG"], "Description": ""}, {"Title": "Image-to-Image Translation with Condotional Adversarial Networks", "Reading Status": "Read", "Date Added": "2018-11-26", "Link": "https://arxiv.org/abs/1611.07004", "Topics": ["Computer Vision", "GAN"], "Description": ""}, {"Title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought", "Reading Status": "Want to Read", "Date Added": "2025-01-13", "Link": "https://arxiv.org/abs/2501.07542", "Topics": [], "Description": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing complex reasoning in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Yet, it struggles in complex spatial reasoning tasks. Nonetheless, human cognition extends beyond language alone, enabling the remarkable capability to think in both words and images. Inspired by this mechanism, we propose a new reasoning paradigm, Multimodal Visualization-of-Thought (MVoT). It enables visual thinking in MLLMs by generating image visualizations of their reasoning traces. To ensure high-quality visualization, we introduce token discrepancy loss into autoregressive MLLMs. This innovation significantly improves both visual coherence and fidelity. We validate this approach through several dynamic spatial reasoning tasks. Experimental results reveal that MVoT demonstrates competitive performance across tasks. Moreover, it exhibits robust and reliable improvements in the most challenging scenarios where CoT fails. Ultimately, MVoT establishes new possibilities for complex reasoning tasks where visual thinking can effectively complement verbal reasoning."}, {"Title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented Generation", "Reading Status": "Reading", "Date Added": "2024-09-10", "Link": "https://arxiv.org/abs/2409.13731", "Topics": [], "Description": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods."}, {"Title": "Learning Transferable Visual Models From Natural Language Supervision", "Reading Status": "Read", "Date Added": "2021-02-26", "Link": "https://arxiv.org/abs/2103.00020", "Topics": ["Computer Vision", "CLIP"], "Description": ""}, {"Title": "LoRA: Low-Rank Adaptation of Large Language Models", "Reading Status": "Read", "Date Added": "2021-10-16", "Link": "https://arxiv.org/abs/2106.09685", "Topics": ["Fine-tuning", "LLM", "LoRA"], "Description": ""}, {"Title": "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge", "Reading Status": "Read", "Date Added": "2024-07-30", "Link": "https://arxiv.org/abs/2407.19594", "Topics": ["LLM", "Judge"], "Description": ""}, {"Title": "MiniMax-01: Scaling Foundation Models with Lightning Attention", "Reading Status": "Want to Read", "Date Added": "2025-01-14", "Link": "https://arxiv.org/abs/2501.08313", "Topics": [], "Description": "We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at https://github.com/MiniMax-AI."}, {"Title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window", "Reading Status": "Read", "Date Added": "2024-07-16", "Link": "https://arxiv.org/abs/2407.11963", "Topics": ["LLM", "Context Window"], "Description": ""}, {"Title": "OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking", "Reading Status": "Want to Read", "Date Added": "2025-01-16", "Link": "https://arxiv.org/abs/2501.09751", "Topics": [], "Description": "Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles."}, {"Title": "OpenAI o1 System Card", "Reading Status": "Want to Read", "Date Added": "2024-09-12", "Link": "", "Topics": ["LLM", "OpenAI", "Technical Report"], "Description": ""}, {"Title": "Phi-4 Technical Report", "Reading Status": "Want to Read", "Date Added": "2024-12-12", "Link": "https://arxiv.org/abs/2412.08905", "Topics": ["LLM", "Technical Report"], "Description": ""}, {"Title": "QLoRA: Efficient Finetuning of Quantized LLMs", "Reading Status": "Read", "Date Added": "2023-05-23", "Link": "https://arxiv.org/abs/2305.14314", "Topics": ["Fine-tuning", "LLM", "LoRA"], "Description": ""}, {"Title": "Sigmoid Loss for Language Image Pre-Training", "Reading Status": "Read", "Date Added": "2023-09-27", "Link": "https://arxiv.org/abs/2303.15343", "Topics": ["Computer Vision", "CLIP"], "Description": ""}, {"Title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference", "Reading Status": "Want to Read", "Date Added": "2024-12-19", "Link": "https://arxiv.org/abs/2412.13663", "Topics": [], "Description": ""}, {"Title": "The Perfect Blend: Redefining RLHF with Mixture of Judges", "Reading Status": "Read", "Date Added": "2024-09-30", "Link": "https://arxiv.org/abs/2409.20370", "Topics": ["LLM", "RLHF", "MOJ"], "Description": "Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives.\nOur empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4% in AlpacaEval-2 (general chat), 12.5% in Arena-Hard (STEM & reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications."}, {"Title": "VideoRAG: Retrieval-Augmented Generation over Video Corpus", "Reading Status": "Want to Read", "Date Added": "2025-01-10", "Link": "https://arxiv.org/abs/2501.05874", "Topics": [], "Description": "Retrieval-Augmented Generation (RAG) is a powerful strategy to address the issue of generating factually incorrect outputs in foundation models by retrieving external knowledge relevant to queries and incorporating it into their generation process. However, existing RAG approaches have primarily focused on textual information, with some recent advancements beginning to consider images, and they largely overlook videos, a rich source of multimodal knowledge capable of representing events, processes, and contextual details more effectively than any other modality. While a few recent studies explore the integration of videos in the response generation process, they either predefine query-associated videos without retrieving them according to queries, or convert videos into the textual descriptions without harnessing their multimodal richness. To tackle these, we introduce VideoRAG, a novel framework that not only dynamically retrieves relevant videos based on their relevance with queries but also utilizes both visual and textual information of videos in the output generation. Further, to operationalize this, our method revolves around the recent advance of Large Video Language Models (LVLMs), which enable the direct processing of video content to represent it for retrieval and seamless integration of the retrieved videos jointly with queries. We experimentally validate the effectiveness of VideoRAG, showcasing that it is superior to relevant baselines."}, {"Title": "YOLOv10: Real-Time End-to-End Object Detection", "Reading Status": "Want to Read", "Date Added": "2024-05-23", "Link": "https://arxiv.org/abs/2405.14458", "Topics": [], "Description": ""}, {"Title": "You Only Look Once: Unified, Real-Time Object Detection", "Reading Status": "Want to Read", "Date Added": "2016-05-09", "Link": "https://arxiv.org/abs/1506.02640", "Topics": ["Computer Vision", "YOLO"], "Description": ""}]